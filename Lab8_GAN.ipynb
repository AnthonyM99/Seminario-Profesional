{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Copia de Copia de 2020_SP1_GAN_students.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true,
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/AnthonyM99/Seminario-Profesional/blob/master/Lab8_GAN.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cpwTxfsGN5Qj",
        "colab_type": "text"
      },
      "source": [
        "# **Copyright Information**\n",
        "\n",
        "####Copyright David Gündisch\n",
        "#####El código original de este tutorial pertenece a David Gündisch. Este código fue modificado agregando comentarios, explicaciones e imágenes para ser una práctica interactiva educativa.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6tHDd2Aye8zf",
        "colab_type": "text"
      },
      "source": [
        "### **Laboratorio 8: Redes Generativas Adversariales**\n",
        "En este laboratorio crearemos una Red Generativa Adversarial para generar nuevas imágenes de dígitos, similares a los dígitos del dataset MNIST.\n",
        "\n",
        "**Nota: recuerde activar la conexión entre Google Colab y Google Drive**\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RVYpcIcnmwFg",
        "colab_type": "code",
        "outputId": "9357745c-8b61-472e-c8ea-93183074ed21",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 122
        }
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Go to this URL in a browser: https://accounts.google.com/o/oauth2/auth?client_id=947318989803-6bn6qk8qdgf4n4g3pfee6491hc0brc4i.apps.googleusercontent.com&redirect_uri=urn%3aietf%3awg%3aoauth%3a2.0%3aoob&response_type=code&scope=email%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdocs.test%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdrive%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdrive.photos.readonly%20https%3a%2f%2fwww.googleapis.com%2fauth%2fpeopleapi.readonly\n",
            "\n",
            "Enter your authorization code:\n",
            "··········\n",
            "Mounted at /content/drive\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5aw5B6jwmrNT",
        "colab_type": "text"
      },
      "source": [
        "# Nueva sección"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dt4Byq00Z0Nt",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from __future__ import absolute_import, division, print_function, unicode_literals\n",
        "\n",
        "# Install TensorFlow\n",
        "try:\n",
        "  # %tensorflow_version only works in Colab.\n",
        "  %tensorflow_version 2.x\n",
        "except Exception:\n",
        "  pass\n",
        "\n",
        "import tensorflow as tf"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HJf_oqFYZ4Xw",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from tensorflow.keras.datasets import mnist\n",
        "from tensorflow.keras.layers import Input, Dense, Reshape, Flatten\n",
        "from tensorflow.keras.layers import BatchNormalization\n",
        "from tensorflow.keras.layers import LeakyReLU\n",
        "from tensorflow.keras.models import Sequential, Model\n",
        "from tensorflow.keras.optimizers import Adam\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Tl0V7Eedu7Yt",
        "colab_type": "text"
      },
      "source": [
        "Las Redes Generativas Adversariales son una forma de generar un modelo utilizando dos redes neurales que compiten una con otra.\n",
        "\n",
        "El **generador** convierte ruido en una imitación de los datos intentando engañar al discriminador.\n",
        "\n",
        "EL **discriminador** intenta identificar los datos reales de los datos falsos creados por el generador.\n",
        "\n",
        "El generador recibe como entrada ruido aleatorio. Solamente el discriminador tiene acceso a los datos de entrenamiento para propósitos de clasificación.\n",
        "El generador mejora el resultado de su salida solamente utilizando la retroalimentación obtenida del discriminador.\n",
        "\n",
        "![texto alternativo](https://www.pngitem.com/pimgs/m/96-965914_a-short-introduction-to-generative-adversarial-networks-generative.png)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SzMXgUhbu0V2",
        "colab_type": "text"
      },
      "source": [
        "La clase GAN() contiene los métodos necesarios para construir nuestra red y entrenarla."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "E1Od-PHQZ6Qv",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class GAN():\n",
        "    def __init__(self):\n",
        "        self.img_rows = 28\n",
        "        self.img_cols = 28\n",
        "        self.channels = 1\n",
        "        self.img_shape = (self.img_rows, self.img_cols, self.channels)\n",
        "        \n",
        "        '''TODO: Especifique la dimensión del espacio latente e.g. 100'''\n",
        "        self.latent_dim = 100\n",
        "        '''TODO: Especifique el optimizador que minimiza la función de perdida'''\n",
        "        optimizer = Adam()\n",
        "        \n",
        "        ### Discriminador de la GAN\n",
        "        self.discriminator = self.build_discriminator()\n",
        "\n",
        "        '''TODO: Coloque la función de perdida y la métrica correcta para el discriminador, ponga atención al trabajo\n",
        "        que está realizando el discriminador. e.g. clasificación multiclase, clasificación binaria, regresión.'''\n",
        "        self.discriminator.compile(loss='binary_crossentropy',\n",
        "                                   optimizer=optimizer,\n",
        "                                   metrics=['accuracy'])\n",
        "        \n",
        "        ### Generador de la GAN\n",
        "        self.generator = self.build_generator()\n",
        "        '''TODO: z es la entrada hacia el generador (vector 1D de ruido). Indique la dimensión de entrada correcta para z.'''\n",
        "        z = Input(shape=(self.latent_dim,))\n",
        "        \n",
        "        '''TODO: el generador recibe ruido como entrada y devuelve una imagen'''\n",
        "        img = self.generator(z)\n",
        "        \n",
        "        '''TODO: No deseamos que la gradiente se propague hacia el discriminador, solamente deseamos propagar las gradientes\n",
        "        hacia el generador'''\n",
        "        self.discriminator.trainable = False\n",
        "\n",
        "        '''TODO: El discriminador recibe la imagen creada por el generador y nos indica si la imagen es verdadera o falsa'''\n",
        "        validity = self.discriminator(img)\n",
        "        \n",
        "        '''TODO': Creamos un modelo con el generador y el discriminador unidos, especifique la entrada para el modelo completo\n",
        "        y la salida correspondiente. '''\n",
        "        self.combined = Model(inputs= z, outputs= validity)\n",
        "        \n",
        "        self.combined.compile(loss='binary_crossentropy', optimizer=optimizer)\n",
        "\n",
        "    def build_generator(self):\n",
        "        ### Este método construye la parte del generador de la GAN\n",
        "\n",
        "        model = Sequential()\n",
        "\n",
        "        model.add(Dense(256, input_dim=self.latent_dim))\n",
        "        model.add(LeakyReLU(alpha=0.2))\n",
        "        model.add(BatchNormalization(momentum=0.8))\n",
        "        \n",
        "        '''TODO: Agregue una capa totalmente conectada con una función de activación LeakyReLU y BatchNormalization.\n",
        "        Agregue una nueva capa totalmente conectada adicional con función de activación LeakyReLU y BatchNormalization\n",
        "        Recuerde que iniciamos nuestra entrada al generador con la dimensión del espacio latente e.g. 100, debemos ir\n",
        "        aumentando las dimensiones de forma similar a la estructura utilizada en la sección 'decoder' en un autoencoder.\n",
        "        Sabiendo esto debe indicar los valores correctos para la cantidad de salidas en cada capa densa.'''\n",
        "        \n",
        "        '''TODO: capa 1'''\n",
        "        '''TODO: capa 2'''\n",
        "        model.add(Dense(512))\n",
        "        model.add(LeakyReLU(alpha=0.2))\n",
        "        model.add(BatchNormalization(momentum=0.8))\n",
        "        model.add(Dense(1024))\n",
        "        model.add(LeakyReLU(alpha=0.2))\n",
        "        model.add(BatchNormalization(momentum=0.8))\n",
        "        \n",
        "        ### La penúltima capa del generador es un vector 1D con dimensión ancho_imagen x alto_imagen e.g.784\n",
        "        \n",
        "        model.add(Dense(np.prod(self.img_shape), activation='tanh'))\n",
        "        \n",
        "        '''TODO: Una imagen de un dígito de MNIST es un objeto 2D, nuestra capa anterior es 1D, debemos pasar de 1D (784,) \n",
        "        a 2D (28,28) utilizando Reshape(new_size)'''\n",
        "        model.add(Reshape(self.img_shape))\n",
        "        \n",
        "        model.summary()\n",
        "        \n",
        "        noise = Input(shape=(self.latent_dim,))\n",
        "        img = model(noise)\n",
        "        \n",
        "        '''TODO': La función build_generator() devuelve solamente la red neural que corresponde a la parte del generador.\n",
        "        Recuerde lo que recibe el generador como entrada y que entrega en la salida.'''\n",
        "        return Model(inputs=noise, outputs=img)\n",
        "\n",
        "    def build_discriminator(self):\n",
        "    \n",
        "        model = Sequential()\n",
        "        ### El discriminador es un clasificador, este clasificador no utiliza CNN, es un clasificador con una FCN, su entrada es un vector 1D\n",
        "        model.add(Flatten(input_shape=self.img_shape))\n",
        "        \n",
        "        '''TODO: agregue dos capas densas con función de activación LeakyRelu'''\n",
        "        '''TODO: capa 1'''\n",
        "        '''TODO: capa 2'''\n",
        "        model.add(Dense(512))\n",
        "        model.add(LeakyReLU(alpha=0.2))\n",
        "        model.add(Dense(256))\n",
        "        model.add(LeakyReLU(alpha=0.2))\n",
        "               \n",
        "        \n",
        "        '''TODO: el discriminador es un clasificador. ¿Es un clasificador binario o debe clasificar mas de 2 clases? es un clasificador binario ya que solo dic si es o no real'''\n",
        "        model.add(Dense(1,activation='sigmoid'))\n",
        "        \n",
        "        model.summary()\n",
        "        \n",
        "        img = Input(shape=self.img_shape)\n",
        "        validity = model(img)\n",
        "        \n",
        "        '''TODO': La función build_discriminator() devuelve solamente la red neural que corresponde a la parte del discriminador.\n",
        "        Recuerde lo que recibe el discriminador como entrada y que entrega en la salida.'''\n",
        "        return Model(inputs=img, outputs=validity)\n",
        "\n",
        "    def train(self, epochs, batch_size=128, sample_interval=50):\n",
        "        ### Método de entrenamiento de la GAN\n",
        "        \n",
        "        ### Se carga el dataset MNIST\n",
        "        (X_train, _), (_, _) = mnist.load_data()\n",
        "        \n",
        "        ### Escalamos el valor de los pixels al rango (-1,1)\n",
        "        X_train = X_train / 127.5 - 1.\n",
        "        \n",
        "        ### El dataset contiene 60k imágenes de 28x28 pixels, los caracteres son blanco y negro, por lo tanto usualmente \n",
        "        ### el shape del dataset no incluye canales. Expandimos nuestro dataset para indicar que solo tenemos 1 canal.\n",
        "        ### dataset shape  = (60000, 28, 28) -> (60000, 28, 28, 1)\n",
        "        X_train = np.expand_dims(X_train, axis=3)\n",
        "        \n",
        "        ### Etiquetas (labels) para enviar al discriminador para las imágenes verdaderas y falsas. \n",
        "        ### imagen verdadera = 1 , imagen falsa = 0\n",
        "        ### valid.shape = (132, 1)   \n",
        "        ### fake.shape(132,1)\n",
        "        valid = np.ones((batch_size, 1))\n",
        "        fake = np.zeros((batch_size, 1))\n",
        "\n",
        "        ### Proceso iterativo de entrenamiento del modelo\n",
        "        for epoch in range(epochs):\n",
        "            \n",
        "            ### Se carga un batch para entrenamiento\n",
        "            idx = np.random.randint(0, X_train.shape[0], batch_size)\n",
        "            imgs = X_train[idx]\n",
        "            \n",
        "            \n",
        "            noise = np.random.normal(0, 1, (batch_size, self.latent_dim))\n",
        "\n",
        "            '''TODO: train_on_batch ejecuta una única actualización de gradiente con un batch de datos\n",
        "                \n",
        "               Entrenamos el generador en el modelo combinado, utilizando la retroalimentación\n",
        "               que nos da el discriminador.         \n",
        "               Realizamos foward_pass a través del modelo combinado generador + discriminador. \n",
        "               pero en backward_pass solo propagamos la gradiente hacia el generador, logramos esto \n",
        "               porque recuerde que anteriormente indicamos que los pesos del discriminador \n",
        "               serán no entrenables en el modelo combinado.\n",
        "               Indicaremos que las etiquetas(labels) para las imágenes creadas por el generador \n",
        "               seran verdaderas e.g 1. (porque deseamos hacer pasarlas por reales)\n",
        "             \n",
        "               En un una iteración el discriminador está entrenado para distinguir \n",
        "               imágenes verdaderas y falsas.\n",
        "               El generador a partir de ruido genera una imagen. En este momento\n",
        "               no sabemos si esta imagen es lo suficiente buena para pasar por real, pero queremos hacerla \n",
        "               pasar por real, por lo tanto como etiqueta indicaremos que esta imagen es verdadera e.g 1.\n",
        "               Deseamos que el generador se acerque lo mas posible a generar imágenes reales.\n",
        "               Posteriormente esta imagen pasa al discriminador a través del modelo combinado y el \n",
        "               discriminador podrá decirnos si es verdadera o falsa.\n",
        "               Utilizando esa información que nos da el discriminador, modificaremos los pesos del generador \n",
        "               para que la próxima iteración podamos acercarnos más a engañar al discriminador.'''\n",
        "            \n",
        "\n",
        "            g_loss = self.combined.train_on_batch(x=noise, y=valid)\n",
        "\n",
        "            '''TODO: Posteriormente de haber realizado una actualización de los pesos del generador, generaremos un batch de\n",
        "               imágenes (falsas) a partir de ruido.''' \n",
        "            gen_imgs = self.generator.predict(noise)\n",
        "\n",
        "            \n",
        "            '''TODO: Entrenamos al discriminador para que aprenda a clasificar \n",
        "               correctamente imágenes verdaderas. Para esto debemos enviarle ejemplos\n",
        "               de imágenes reales. Las imágenes verdaderas son las imágenes\n",
        "               que se obtienen del dataset MNIST. Debemos indicarle al discriminador \n",
        "               que estas imágenes son verdaderas, esto lo hacemos asignando a cada imagen una etiqueta\n",
        "               verdadera e.g 1'''\n",
        "            d_loss_real = self.discriminator.train_on_batch(x=imgs, y=valid)\n",
        "\n",
        "            '''TODO: Entrenamos al discriminador para que aprenda a identificar\n",
        "               correctamente imágenes falsas. Para esto debemos enviarle ejemplos\n",
        "               de imágenes falsas. Las imágenes falsas son las imágenes\n",
        "               que se obtienen del generador. Debemos indicarle al discriminador \n",
        "               que estas imágenes son falsas, esto lo hacemos asignando a cada imagen una etiqueta\n",
        "               falsa e.g 0'''\n",
        "            d_loss_fake = self.discriminator.train_on_batch(x=gen_imgs, y=fake)\n",
        "\n",
        "            ### valor de perdida del discriminador\n",
        "            d_loss = 0.5 * np.add(d_loss_real, d_loss_fake)\n",
        "            \n",
        "            if epoch % 50 == 0:\n",
        "                print(\"%d [D loss: %f, acc.: %.2f%%] [G loss: %f]\" % (epoch, d_loss[0], 100 * d_loss[1], g_loss))\n",
        "            if epoch % sample_interval == 0:\n",
        "                self.sample_images(epoch)\n",
        "\n",
        "    def sample_images(self, epoch):\n",
        "        ### Función que crea un archivo .png con ejemplos de imágenes nuevas generadas por\n",
        "        ### el generador\n",
        "        r, c = 5, 5\n",
        "        noise = np.random.normal(0, 1, (r * c, self.latent_dim))\n",
        "        gen_imgs = self.generator.predict(noise)\n",
        "        gen_imgs = 0.5 * gen_imgs + 0.5\n",
        "        fig, axs = plt.subplots(r, c)\n",
        "        cnt = 0\n",
        "        for i in range(r):\n",
        "            for j in range(c):\n",
        "                axs[i, j].imshow(gen_imgs[cnt, :, :, 0], cmap='gray')\n",
        "                axs[i, j].axis('off')\n",
        "                cnt += 1\n",
        "        fig.savefig(\"/content/drive/My Drive/%d.png\" % epoch)\n",
        "        plt.close()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "r68GIhOuUiHM",
        "colab_type": "code",
        "outputId": "685aa34f-5cb7-4921-ed39-5d259463a2aa",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "### Al finalizar el entrenamiento vea las imágenes resultantes \n",
        "### creadas por el generador a través de cada iteración en su directorio de Google Drive.\n",
        "\n",
        "gan = GAN()\n",
        "gan.train(epochs=100000, batch_size=132, sample_interval=10000)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Model: \"sequential_17\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "flatten_10 (Flatten)         (None, 784)               0         \n",
            "_________________________________________________________________\n",
            "dense_55 (Dense)             (None, 512)               401920    \n",
            "_________________________________________________________________\n",
            "leaky_re_lu_41 (LeakyReLU)   (None, 512)               0         \n",
            "_________________________________________________________________\n",
            "dense_56 (Dense)             (None, 256)               131328    \n",
            "_________________________________________________________________\n",
            "leaky_re_lu_42 (LeakyReLU)   (None, 256)               0         \n",
            "_________________________________________________________________\n",
            "dense_57 (Dense)             (None, 1)                 257       \n",
            "=================================================================\n",
            "Total params: 533,505\n",
            "Trainable params: 533,505\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n",
            "Model: \"sequential_18\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "dense_58 (Dense)             (None, 256)               25856     \n",
            "_________________________________________________________________\n",
            "leaky_re_lu_43 (LeakyReLU)   (None, 256)               0         \n",
            "_________________________________________________________________\n",
            "batch_normalization_21 (Batc (None, 256)               1024      \n",
            "_________________________________________________________________\n",
            "dense_59 (Dense)             (None, 512)               131584    \n",
            "_________________________________________________________________\n",
            "leaky_re_lu_44 (LeakyReLU)   (None, 512)               0         \n",
            "_________________________________________________________________\n",
            "batch_normalization_22 (Batc (None, 512)               2048      \n",
            "_________________________________________________________________\n",
            "dense_60 (Dense)             (None, 1024)              525312    \n",
            "_________________________________________________________________\n",
            "leaky_re_lu_45 (LeakyReLU)   (None, 1024)              0         \n",
            "_________________________________________________________________\n",
            "batch_normalization_23 (Batc (None, 1024)              4096      \n",
            "_________________________________________________________________\n",
            "dense_61 (Dense)             (None, 784)               803600    \n",
            "_________________________________________________________________\n",
            "reshape_3 (Reshape)          (None, 28, 28, 1)         0         \n",
            "=================================================================\n",
            "Total params: 1,493,520\n",
            "Trainable params: 1,489,936\n",
            "Non-trainable params: 3,584\n",
            "_________________________________________________________________\n",
            "0 [D loss: 1.155026, acc.: 1.52%] [G loss: 0.823104]\n",
            "50 [D loss: 0.000905, acc.: 100.00%] [G loss: 11.215343]\n",
            "100 [D loss: 0.026238, acc.: 99.24%] [G loss: 16.192146]\n",
            "150 [D loss: 0.011700, acc.: 100.00%] [G loss: 12.120207]\n",
            "200 [D loss: 1.607087, acc.: 71.97%] [G loss: 12.367279]\n",
            "250 [D loss: 0.038616, acc.: 98.86%] [G loss: 9.324727]\n",
            "300 [D loss: 0.002525, acc.: 100.00%] [G loss: 8.362286]\n",
            "350 [D loss: 0.075448, acc.: 96.59%] [G loss: 41.964066]\n",
            "400 [D loss: 0.017580, acc.: 100.00%] [G loss: 7.650675]\n",
            "450 [D loss: 0.015295, acc.: 100.00%] [G loss: 5.557316]\n",
            "500 [D loss: 0.004258, acc.: 100.00%] [G loss: 8.412253]\n",
            "550 [D loss: 0.056479, acc.: 99.24%] [G loss: 5.165947]\n",
            "600 [D loss: 0.040691, acc.: 99.62%] [G loss: 5.531157]\n",
            "650 [D loss: 0.022088, acc.: 99.62%] [G loss: 5.440966]\n",
            "700 [D loss: 0.004919, acc.: 100.00%] [G loss: 6.865932]\n",
            "750 [D loss: 0.022410, acc.: 99.24%] [G loss: 8.273734]\n",
            "800 [D loss: 0.007655, acc.: 100.00%] [G loss: 8.531539]\n",
            "850 [D loss: 0.026643, acc.: 100.00%] [G loss: 5.660844]\n",
            "900 [D loss: 0.472699, acc.: 80.68%] [G loss: 6.598438]\n",
            "950 [D loss: 0.025713, acc.: 99.62%] [G loss: 8.038822]\n",
            "1000 [D loss: 0.011056, acc.: 100.00%] [G loss: 5.987752]\n",
            "1050 [D loss: 0.025823, acc.: 100.00%] [G loss: 6.088510]\n",
            "1100 [D loss: 0.003027, acc.: 100.00%] [G loss: 7.189301]\n",
            "1150 [D loss: 0.036011, acc.: 99.24%] [G loss: 6.585856]\n",
            "1200 [D loss: 0.008785, acc.: 99.62%] [G loss: 7.112728]\n",
            "1250 [D loss: 0.013279, acc.: 100.00%] [G loss: 6.095440]\n",
            "1300 [D loss: 0.017393, acc.: 100.00%] [G loss: 5.852520]\n",
            "1350 [D loss: 0.007115, acc.: 100.00%] [G loss: 9.321595]\n",
            "1400 [D loss: 0.010713, acc.: 100.00%] [G loss: 8.463206]\n",
            "1450 [D loss: 0.016173, acc.: 99.62%] [G loss: 6.632239]\n",
            "1500 [D loss: 0.020482, acc.: 100.00%] [G loss: 7.690401]\n",
            "1550 [D loss: 0.012387, acc.: 100.00%] [G loss: 12.914973]\n",
            "1600 [D loss: 0.006579, acc.: 100.00%] [G loss: 7.894345]\n",
            "1650 [D loss: 0.003924, acc.: 100.00%] [G loss: 8.037293]\n",
            "1700 [D loss: 0.007280, acc.: 100.00%] [G loss: 5.845961]\n",
            "1750 [D loss: 0.013974, acc.: 99.62%] [G loss: 6.711866]\n",
            "1800 [D loss: 0.019121, acc.: 99.62%] [G loss: 8.801548]\n",
            "1850 [D loss: 0.003246, acc.: 100.00%] [G loss: 8.771865]\n",
            "1900 [D loss: 0.004014, acc.: 100.00%] [G loss: 14.548078]\n",
            "1950 [D loss: 0.000021, acc.: 100.00%] [G loss: 24.104542]\n",
            "2000 [D loss: 0.009334, acc.: 100.00%] [G loss: 4.959297]\n",
            "2050 [D loss: 0.013254, acc.: 99.62%] [G loss: 7.055751]\n",
            "2100 [D loss: 0.003074, acc.: 100.00%] [G loss: 9.515585]\n",
            "2150 [D loss: 0.005888, acc.: 100.00%] [G loss: 8.080512]\n",
            "2200 [D loss: 0.005818, acc.: 99.62%] [G loss: 8.782150]\n",
            "2250 [D loss: 0.012086, acc.: 99.62%] [G loss: 8.511755]\n",
            "2300 [D loss: 0.002640, acc.: 100.00%] [G loss: 8.712871]\n",
            "2350 [D loss: 0.035577, acc.: 98.48%] [G loss: 7.082178]\n",
            "2400 [D loss: 0.044244, acc.: 98.86%] [G loss: 6.006100]\n",
            "2450 [D loss: 0.028186, acc.: 100.00%] [G loss: 7.990149]\n",
            "2500 [D loss: 0.023285, acc.: 99.24%] [G loss: 11.749253]\n",
            "2550 [D loss: 0.006648, acc.: 99.62%] [G loss: 7.677764]\n",
            "2600 [D loss: 0.012541, acc.: 99.62%] [G loss: 7.825814]\n",
            "2650 [D loss: 0.058666, acc.: 97.73%] [G loss: 14.743327]\n",
            "2700 [D loss: 0.029860, acc.: 99.24%] [G loss: 7.404469]\n",
            "2750 [D loss: 0.026501, acc.: 99.24%] [G loss: 9.056522]\n",
            "2800 [D loss: 0.002712, acc.: 100.00%] [G loss: 7.282401]\n",
            "2850 [D loss: 0.002847, acc.: 100.00%] [G loss: 9.849476]\n",
            "2900 [D loss: 0.010783, acc.: 100.00%] [G loss: 6.869510]\n",
            "2950 [D loss: 0.006136, acc.: 100.00%] [G loss: 9.093499]\n",
            "3000 [D loss: 0.006901, acc.: 100.00%] [G loss: 6.457047]\n",
            "3050 [D loss: 0.007493, acc.: 100.00%] [G loss: 6.829961]\n",
            "3100 [D loss: 0.016765, acc.: 99.62%] [G loss: 6.135453]\n",
            "3150 [D loss: 0.012235, acc.: 100.00%] [G loss: 7.257708]\n",
            "3200 [D loss: 0.005967, acc.: 100.00%] [G loss: 7.975390]\n",
            "3250 [D loss: 0.666200, acc.: 84.47%] [G loss: 24.049835]\n",
            "3300 [D loss: 0.003219, acc.: 100.00%] [G loss: 7.660245]\n",
            "3350 [D loss: 0.006473, acc.: 99.62%] [G loss: 7.420910]\n",
            "3400 [D loss: 0.006286, acc.: 100.00%] [G loss: 7.813599]\n",
            "3450 [D loss: 0.008953, acc.: 100.00%] [G loss: 7.009319]\n",
            "3500 [D loss: 0.001050, acc.: 100.00%] [G loss: 10.390740]\n",
            "3550 [D loss: 0.004777, acc.: 100.00%] [G loss: 7.581271]\n",
            "3600 [D loss: 0.003529, acc.: 100.00%] [G loss: 7.854423]\n",
            "3650 [D loss: 0.005207, acc.: 100.00%] [G loss: 8.132308]\n",
            "3700 [D loss: 0.002153, acc.: 100.00%] [G loss: 10.119347]\n",
            "3750 [D loss: 0.003279, acc.: 100.00%] [G loss: 7.928820]\n",
            "3800 [D loss: 0.010513, acc.: 99.62%] [G loss: 11.962585]\n",
            "3850 [D loss: 0.008089, acc.: 100.00%] [G loss: 9.676960]\n",
            "3900 [D loss: 0.021548, acc.: 99.62%] [G loss: 6.648893]\n",
            "3950 [D loss: 0.005745, acc.: 100.00%] [G loss: 5.910048]\n",
            "4000 [D loss: 0.005313, acc.: 100.00%] [G loss: 7.357446]\n",
            "4050 [D loss: 0.007317, acc.: 100.00%] [G loss: 7.203076]\n",
            "4100 [D loss: 0.003826, acc.: 100.00%] [G loss: 6.832503]\n",
            "4150 [D loss: 0.011469, acc.: 99.62%] [G loss: 6.654490]\n",
            "4200 [D loss: 0.074931, acc.: 96.59%] [G loss: 7.658578]\n",
            "4250 [D loss: 0.018434, acc.: 99.62%] [G loss: 5.950689]\n",
            "4300 [D loss: 0.006915, acc.: 100.00%] [G loss: 6.888927]\n",
            "4350 [D loss: 0.008190, acc.: 100.00%] [G loss: 9.413011]\n",
            "4400 [D loss: 0.024061, acc.: 98.86%] [G loss: 6.812967]\n",
            "4450 [D loss: 0.001375, acc.: 100.00%] [G loss: 8.733770]\n",
            "4500 [D loss: 0.067079, acc.: 96.97%] [G loss: 8.041678]\n",
            "4550 [D loss: 0.011632, acc.: 100.00%] [G loss: 6.152730]\n",
            "4600 [D loss: 0.010482, acc.: 100.00%] [G loss: 7.816778]\n",
            "4650 [D loss: 0.007972, acc.: 99.62%] [G loss: 8.030078]\n",
            "4700 [D loss: 0.003935, acc.: 100.00%] [G loss: 8.559322]\n",
            "4750 [D loss: 0.011887, acc.: 100.00%] [G loss: 9.545830]\n",
            "4800 [D loss: 0.041331, acc.: 98.48%] [G loss: 15.234725]\n",
            "4850 [D loss: 0.012745, acc.: 100.00%] [G loss: 6.561967]\n",
            "4900 [D loss: 0.006498, acc.: 100.00%] [G loss: 7.406542]\n",
            "4950 [D loss: 0.007053, acc.: 100.00%] [G loss: 6.937651]\n",
            "5000 [D loss: 0.006314, acc.: 100.00%] [G loss: 7.487633]\n",
            "5050 [D loss: 0.013634, acc.: 100.00%] [G loss: 6.978393]\n",
            "5100 [D loss: 0.007436, acc.: 100.00%] [G loss: 7.857125]\n",
            "5150 [D loss: 0.014233, acc.: 100.00%] [G loss: 6.813949]\n",
            "5200 [D loss: 0.009879, acc.: 100.00%] [G loss: 8.281217]\n",
            "5250 [D loss: 0.012908, acc.: 99.24%] [G loss: 10.220562]\n",
            "5300 [D loss: 0.006212, acc.: 99.62%] [G loss: 7.389079]\n",
            "5350 [D loss: 0.006234, acc.: 100.00%] [G loss: 7.856642]\n",
            "5400 [D loss: 0.002015, acc.: 100.00%] [G loss: 14.437368]\n",
            "5450 [D loss: 0.009543, acc.: 100.00%] [G loss: 7.153871]\n",
            "5500 [D loss: 0.038461, acc.: 99.24%] [G loss: 11.287272]\n",
            "5550 [D loss: 0.019389, acc.: 99.24%] [G loss: 11.581246]\n",
            "5600 [D loss: 0.024775, acc.: 99.62%] [G loss: 6.176028]\n",
            "5650 [D loss: 0.008798, acc.: 100.00%] [G loss: 6.768700]\n",
            "5700 [D loss: 0.012497, acc.: 100.00%] [G loss: 6.960816]\n",
            "5750 [D loss: 0.004947, acc.: 100.00%] [G loss: 9.520414]\n",
            "5800 [D loss: 0.009661, acc.: 100.00%] [G loss: 8.316088]\n",
            "5850 [D loss: 0.001073, acc.: 100.00%] [G loss: 8.618206]\n",
            "5900 [D loss: 0.011159, acc.: 99.24%] [G loss: 12.449392]\n",
            "5950 [D loss: 0.017234, acc.: 100.00%] [G loss: 5.706231]\n",
            "6000 [D loss: 0.010918, acc.: 100.00%] [G loss: 8.294383]\n",
            "6050 [D loss: 0.008180, acc.: 100.00%] [G loss: 7.933729]\n",
            "6100 [D loss: 0.008681, acc.: 100.00%] [G loss: 7.331851]\n",
            "6150 [D loss: 0.008116, acc.: 99.62%] [G loss: 8.832985]\n",
            "6200 [D loss: 0.032098, acc.: 98.86%] [G loss: 7.503748]\n",
            "6250 [D loss: 0.012630, acc.: 99.62%] [G loss: 12.250422]\n",
            "6300 [D loss: 0.009436, acc.: 100.00%] [G loss: 6.670799]\n",
            "6350 [D loss: 0.004110, acc.: 100.00%] [G loss: 6.940272]\n",
            "6400 [D loss: 0.008838, acc.: 100.00%] [G loss: 6.775157]\n",
            "6450 [D loss: 0.005430, acc.: 100.00%] [G loss: 8.624782]\n",
            "6500 [D loss: 0.002318, acc.: 100.00%] [G loss: 8.289946]\n",
            "6550 [D loss: 0.061166, acc.: 98.11%] [G loss: 5.674906]\n",
            "6600 [D loss: 0.028137, acc.: 99.24%] [G loss: 7.049496]\n",
            "6650 [D loss: 0.039945, acc.: 98.11%] [G loss: 8.146214]\n",
            "6700 [D loss: 0.024382, acc.: 99.62%] [G loss: 5.037655]\n",
            "6750 [D loss: 0.008695, acc.: 100.00%] [G loss: 5.477028]\n",
            "6800 [D loss: 0.010832, acc.: 100.00%] [G loss: 7.257869]\n",
            "6850 [D loss: 0.015392, acc.: 100.00%] [G loss: 6.671685]\n",
            "6900 [D loss: 0.060799, acc.: 99.24%] [G loss: 4.620688]\n",
            "6950 [D loss: 0.021787, acc.: 100.00%] [G loss: 4.694011]\n",
            "7000 [D loss: 0.006309, acc.: 100.00%] [G loss: 6.413111]\n",
            "7050 [D loss: 0.015519, acc.: 99.62%] [G loss: 8.209464]\n",
            "7100 [D loss: 0.051386, acc.: 98.48%] [G loss: 5.395561]\n",
            "7150 [D loss: 0.046343, acc.: 99.62%] [G loss: 5.282493]\n",
            "7200 [D loss: 0.018159, acc.: 100.00%] [G loss: 6.447438]\n",
            "7250 [D loss: 0.015373, acc.: 99.62%] [G loss: 7.589748]\n",
            "7300 [D loss: 0.007409, acc.: 100.00%] [G loss: 7.447633]\n",
            "7350 [D loss: 0.025590, acc.: 98.86%] [G loss: 11.905339]\n",
            "7400 [D loss: 0.040990, acc.: 100.00%] [G loss: 4.346794]\n",
            "7450 [D loss: 0.020084, acc.: 100.00%] [G loss: 6.385044]\n",
            "7500 [D loss: 0.018716, acc.: 99.62%] [G loss: 5.620838]\n",
            "7550 [D loss: 0.015206, acc.: 100.00%] [G loss: 5.636215]\n",
            "7600 [D loss: 0.009946, acc.: 100.00%] [G loss: 7.494076]\n",
            "7650 [D loss: 0.010551, acc.: 99.62%] [G loss: 8.312033]\n",
            "7700 [D loss: 0.014420, acc.: 100.00%] [G loss: 6.495110]\n",
            "7750 [D loss: 0.026500, acc.: 99.24%] [G loss: 8.048042]\n",
            "7800 [D loss: 0.009417, acc.: 100.00%] [G loss: 6.426995]\n",
            "7850 [D loss: 0.005400, acc.: 100.00%] [G loss: 9.725425]\n",
            "7900 [D loss: 0.017595, acc.: 99.62%] [G loss: 7.359933]\n",
            "7950 [D loss: 0.005799, acc.: 100.00%] [G loss: 8.914783]\n",
            "8000 [D loss: 0.018416, acc.: 99.62%] [G loss: 5.442940]\n",
            "8050 [D loss: 0.051408, acc.: 98.86%] [G loss: 5.116566]\n",
            "8100 [D loss: 0.028322, acc.: 99.62%] [G loss: 6.065569]\n",
            "8150 [D loss: 0.020743, acc.: 100.00%] [G loss: 7.649592]\n",
            "8200 [D loss: 0.007842, acc.: 100.00%] [G loss: 14.624791]\n",
            "8250 [D loss: 0.041468, acc.: 99.62%] [G loss: 5.248076]\n",
            "8300 [D loss: 0.019819, acc.: 99.62%] [G loss: 5.416102]\n",
            "8350 [D loss: 0.023503, acc.: 99.24%] [G loss: 8.914386]\n",
            "8400 [D loss: 0.030240, acc.: 98.86%] [G loss: 10.416992]\n",
            "8450 [D loss: 0.002209, acc.: 100.00%] [G loss: 7.937950]\n",
            "8500 [D loss: 0.002926, acc.: 100.00%] [G loss: 7.048922]\n",
            "8550 [D loss: 0.001821, acc.: 100.00%] [G loss: 7.040145]\n",
            "8600 [D loss: 0.005528, acc.: 100.00%] [G loss: 7.552767]\n",
            "8650 [D loss: 0.114007, acc.: 95.45%] [G loss: 5.606760]\n",
            "8700 [D loss: 0.021966, acc.: 100.00%] [G loss: 4.876864]\n",
            "8750 [D loss: 0.019739, acc.: 99.62%] [G loss: 7.427753]\n",
            "8800 [D loss: 0.011206, acc.: 100.00%] [G loss: 7.711399]\n",
            "8850 [D loss: 0.013337, acc.: 100.00%] [G loss: 6.611961]\n",
            "8900 [D loss: 0.014442, acc.: 99.62%] [G loss: 8.564962]\n",
            "8950 [D loss: 0.053352, acc.: 98.86%] [G loss: 6.488323]\n",
            "9000 [D loss: 0.013527, acc.: 100.00%] [G loss: 6.179835]\n",
            "9050 [D loss: 0.006613, acc.: 100.00%] [G loss: 6.133246]\n",
            "9100 [D loss: 0.025864, acc.: 98.48%] [G loss: 11.039780]\n",
            "9150 [D loss: 0.036642, acc.: 99.24%] [G loss: 3.916549]\n",
            "9200 [D loss: 0.022720, acc.: 99.62%] [G loss: 5.630188]\n",
            "9250 [D loss: 0.014820, acc.: 100.00%] [G loss: 6.027874]\n",
            "9300 [D loss: 0.013880, acc.: 100.00%] [G loss: 5.752553]\n",
            "9350 [D loss: 0.010092, acc.: 100.00%] [G loss: 6.106117]\n",
            "9400 [D loss: 0.013629, acc.: 100.00%] [G loss: 7.646652]\n",
            "9450 [D loss: 1.116724, acc.: 73.48%] [G loss: 4.470102]\n",
            "9500 [D loss: 0.025908, acc.: 99.62%] [G loss: 5.903448]\n",
            "9550 [D loss: 0.021761, acc.: 99.62%] [G loss: 6.519864]\n",
            "9600 [D loss: 0.006780, acc.: 99.62%] [G loss: 7.290125]\n",
            "9650 [D loss: 0.003335, acc.: 100.00%] [G loss: 8.736478]\n",
            "9700 [D loss: 0.056825, acc.: 97.73%] [G loss: 5.791574]\n",
            "9750 [D loss: 0.009997, acc.: 100.00%] [G loss: 6.239583]\n",
            "9800 [D loss: 0.007647, acc.: 100.00%] [G loss: 6.183446]\n",
            "9850 [D loss: 0.018211, acc.: 99.62%] [G loss: 7.840386]\n",
            "9900 [D loss: 0.007656, acc.: 100.00%] [G loss: 9.131094]\n",
            "9950 [D loss: 0.007506, acc.: 100.00%] [G loss: 5.868876]\n",
            "10000 [D loss: 0.015030, acc.: 99.62%] [G loss: 7.706638]\n",
            "10050 [D loss: 0.007887, acc.: 100.00%] [G loss: 7.502968]\n",
            "10100 [D loss: 0.045522, acc.: 97.35%] [G loss: 13.820161]\n",
            "10150 [D loss: 0.015789, acc.: 100.00%] [G loss: 5.743477]\n",
            "10200 [D loss: 0.009689, acc.: 100.00%] [G loss: 5.881088]\n",
            "10250 [D loss: 0.007745, acc.: 99.62%] [G loss: 7.190689]\n",
            "10300 [D loss: 0.042820, acc.: 99.24%] [G loss: 11.138718]\n",
            "10350 [D loss: 0.036633, acc.: 99.24%] [G loss: 5.287987]\n",
            "10400 [D loss: 0.020759, acc.: 99.62%] [G loss: 8.308372]\n",
            "10450 [D loss: 0.019219, acc.: 99.24%] [G loss: 9.958229]\n",
            "10500 [D loss: 0.028291, acc.: 99.62%] [G loss: 4.572861]\n",
            "10550 [D loss: 0.022990, acc.: 99.62%] [G loss: 5.873301]\n",
            "10600 [D loss: 0.012515, acc.: 100.00%] [G loss: 5.906442]\n",
            "10650 [D loss: 0.008874, acc.: 100.00%] [G loss: 7.828023]\n",
            "10700 [D loss: 0.032474, acc.: 98.86%] [G loss: 7.697227]\n",
            "10750 [D loss: 7.320236, acc.: 51.14%] [G loss: 53.960430]\n",
            "10800 [D loss: 0.047218, acc.: 100.00%] [G loss: 5.053738]\n",
            "10850 [D loss: 0.014627, acc.: 100.00%] [G loss: 5.933236]\n",
            "10900 [D loss: 0.022674, acc.: 98.86%] [G loss: 7.492531]\n",
            "10950 [D loss: 0.007696, acc.: 100.00%] [G loss: 7.349689]\n",
            "11000 [D loss: 0.005822, acc.: 100.00%] [G loss: 6.761234]\n",
            "11050 [D loss: 0.010131, acc.: 100.00%] [G loss: 8.314444]\n",
            "11100 [D loss: 0.023254, acc.: 99.62%] [G loss: 8.961585]\n",
            "11150 [D loss: 0.017424, acc.: 98.86%] [G loss: 11.521297]\n",
            "11200 [D loss: 0.007152, acc.: 100.00%] [G loss: 18.434322]\n",
            "11250 [D loss: 0.022790, acc.: 100.00%] [G loss: 6.444874]\n",
            "11300 [D loss: 0.010500, acc.: 100.00%] [G loss: 7.766502]\n",
            "11350 [D loss: 0.013795, acc.: 99.62%] [G loss: 7.146907]\n",
            "11400 [D loss: 0.002543, acc.: 100.00%] [G loss: 7.833441]\n",
            "11450 [D loss: 0.003715, acc.: 100.00%] [G loss: 13.323871]\n",
            "11500 [D loss: 0.005091, acc.: 100.00%] [G loss: 11.440059]\n",
            "11550 [D loss: 0.011404, acc.: 99.62%] [G loss: 7.491801]\n",
            "11600 [D loss: 0.020669, acc.: 99.62%] [G loss: 6.760767]\n",
            "11650 [D loss: 0.014201, acc.: 99.62%] [G loss: 5.843332]\n",
            "11700 [D loss: 0.007079, acc.: 100.00%] [G loss: 7.518208]\n",
            "11750 [D loss: 0.027442, acc.: 98.86%] [G loss: 6.565988]\n",
            "11800 [D loss: 0.036608, acc.: 98.86%] [G loss: 5.907332]\n",
            "11850 [D loss: 0.028017, acc.: 100.00%] [G loss: 4.857323]\n",
            "11900 [D loss: 0.019736, acc.: 99.62%] [G loss: 6.297949]\n",
            "11950 [D loss: 0.010127, acc.: 100.00%] [G loss: 6.592277]\n",
            "12000 [D loss: 0.019632, acc.: 99.62%] [G loss: 7.544163]\n",
            "12050 [D loss: 0.097165, acc.: 94.32%] [G loss: 17.867723]\n",
            "12100 [D loss: 0.016091, acc.: 99.62%] [G loss: 6.465008]\n",
            "12150 [D loss: 0.291322, acc.: 87.50%] [G loss: 4.464327]\n",
            "12200 [D loss: 0.033803, acc.: 100.00%] [G loss: 5.664434]\n",
            "12250 [D loss: 0.011422, acc.: 100.00%] [G loss: 7.451561]\n",
            "12300 [D loss: 0.030706, acc.: 99.24%] [G loss: 5.853191]\n",
            "12350 [D loss: 0.029568, acc.: 99.24%] [G loss: 5.181250]\n",
            "12400 [D loss: 0.024426, acc.: 99.62%] [G loss: 6.429318]\n",
            "12450 [D loss: 0.021697, acc.: 99.62%] [G loss: 6.195260]\n",
            "12500 [D loss: 0.023409, acc.: 98.86%] [G loss: 5.940097]\n",
            "12550 [D loss: 0.332812, acc.: 95.83%] [G loss: 26.815289]\n",
            "12600 [D loss: 0.044673, acc.: 98.86%] [G loss: 5.564615]\n",
            "12650 [D loss: 0.021981, acc.: 99.62%] [G loss: 7.270731]\n",
            "12700 [D loss: 0.026663, acc.: 98.86%] [G loss: 6.282789]\n",
            "12750 [D loss: 0.022864, acc.: 99.24%] [G loss: 7.320819]\n",
            "12800 [D loss: 0.029363, acc.: 99.62%] [G loss: 4.779630]\n",
            "12850 [D loss: 0.030379, acc.: 99.62%] [G loss: 5.224293]\n",
            "12900 [D loss: 0.017776, acc.: 99.62%] [G loss: 5.738685]\n",
            "12950 [D loss: 0.021162, acc.: 99.62%] [G loss: 6.486929]\n",
            "13000 [D loss: 0.015134, acc.: 99.62%] [G loss: 6.949950]\n",
            "13050 [D loss: 0.009803, acc.: 100.00%] [G loss: 8.867020]\n",
            "13100 [D loss: 0.021697, acc.: 99.62%] [G loss: 7.429971]\n",
            "13150 [D loss: 0.152550, acc.: 92.80%] [G loss: 5.129689]\n",
            "13200 [D loss: 0.021446, acc.: 99.62%] [G loss: 5.242527]\n",
            "13250 [D loss: 0.014410, acc.: 100.00%] [G loss: 6.876061]\n",
            "13300 [D loss: 0.017440, acc.: 100.00%] [G loss: 6.825280]\n",
            "13350 [D loss: 0.013594, acc.: 100.00%] [G loss: 6.010323]\n",
            "13400 [D loss: 0.016932, acc.: 99.24%] [G loss: 8.972103]\n",
            "13450 [D loss: 0.016118, acc.: 98.86%] [G loss: 9.025330]\n",
            "13500 [D loss: 1.481143, acc.: 68.18%] [G loss: 0.184139]\n",
            "13550 [D loss: 0.036607, acc.: 99.62%] [G loss: 6.569321]\n",
            "13600 [D loss: 0.043628, acc.: 98.86%] [G loss: 5.375104]\n",
            "13650 [D loss: 0.323644, acc.: 89.77%] [G loss: 16.572519]\n",
            "13700 [D loss: 0.036092, acc.: 99.24%] [G loss: 7.043921]\n",
            "13750 [D loss: 0.012902, acc.: 100.00%] [G loss: 7.075499]\n",
            "13800 [D loss: 0.027947, acc.: 99.24%] [G loss: 7.703513]\n",
            "13850 [D loss: 0.029503, acc.: 98.86%] [G loss: 8.120311]\n",
            "13900 [D loss: 0.006681, acc.: 100.00%] [G loss: 7.482156]\n",
            "13950 [D loss: 0.017573, acc.: 99.62%] [G loss: 11.045289]\n",
            "14000 [D loss: 0.007231, acc.: 99.62%] [G loss: 8.038529]\n",
            "14050 [D loss: 0.004269, acc.: 100.00%] [G loss: 10.383693]\n",
            "14100 [D loss: 0.008961, acc.: 100.00%] [G loss: 7.539576]\n",
            "14150 [D loss: 0.002654, acc.: 100.00%] [G loss: 12.862578]\n",
            "14200 [D loss: 0.005878, acc.: 99.62%] [G loss: 9.692952]\n",
            "14250 [D loss: 0.025206, acc.: 98.86%] [G loss: 11.549644]\n",
            "14300 [D loss: 0.014398, acc.: 100.00%] [G loss: 9.778774]\n",
            "14350 [D loss: 0.040095, acc.: 98.86%] [G loss: 7.532847]\n",
            "14400 [D loss: 0.011484, acc.: 99.62%] [G loss: 7.440846]\n",
            "14450 [D loss: 0.025155, acc.: 98.48%] [G loss: 6.835899]\n",
            "14500 [D loss: 0.005889, acc.: 100.00%] [G loss: 8.876377]\n",
            "14550 [D loss: 0.008053, acc.: 100.00%] [G loss: 8.323380]\n",
            "14600 [D loss: 0.008337, acc.: 100.00%] [G loss: 8.357682]\n",
            "14650 [D loss: 0.051888, acc.: 97.73%] [G loss: 12.096318]\n",
            "14700 [D loss: 0.047376, acc.: 98.11%] [G loss: 5.333943]\n",
            "14750 [D loss: 0.017795, acc.: 99.62%] [G loss: 6.442379]\n",
            "14800 [D loss: 0.012492, acc.: 99.62%] [G loss: 7.211630]\n",
            "14850 [D loss: 0.011723, acc.: 100.00%] [G loss: 6.763281]\n",
            "14900 [D loss: 0.009972, acc.: 100.00%] [G loss: 7.978795]\n",
            "14950 [D loss: 0.010472, acc.: 100.00%] [G loss: 8.137811]\n",
            "15000 [D loss: 0.007514, acc.: 100.00%] [G loss: 6.923272]\n",
            "15050 [D loss: 0.018796, acc.: 99.62%] [G loss: 11.605879]\n",
            "15100 [D loss: 0.016168, acc.: 100.00%] [G loss: 6.640219]\n",
            "15150 [D loss: 0.014973, acc.: 100.00%] [G loss: 7.369867]\n",
            "15200 [D loss: 0.021595, acc.: 99.24%] [G loss: 8.725475]\n",
            "15250 [D loss: 4.996324, acc.: 51.52%] [G loss: 48.754082]\n",
            "15300 [D loss: 0.071309, acc.: 97.73%] [G loss: 6.721372]\n",
            "15350 [D loss: 0.022273, acc.: 99.62%] [G loss: 5.858456]\n",
            "15400 [D loss: 0.022723, acc.: 99.62%] [G loss: 7.738132]\n",
            "15450 [D loss: 0.007860, acc.: 100.00%] [G loss: 7.808092]\n",
            "15500 [D loss: 0.002039, acc.: 100.00%] [G loss: 7.502459]\n",
            "15550 [D loss: 0.125103, acc.: 95.45%] [G loss: 12.184932]\n",
            "15600 [D loss: 4.102580, acc.: 50.76%] [G loss: 0.000037]\n",
            "15650 [D loss: 0.050566, acc.: 99.24%] [G loss: 5.876248]\n",
            "15700 [D loss: 0.077108, acc.: 97.35%] [G loss: 5.946370]\n",
            "15750 [D loss: 0.048447, acc.: 98.48%] [G loss: 5.923757]\n",
            "15800 [D loss: 0.052909, acc.: 98.11%] [G loss: 6.613719]\n",
            "15850 [D loss: 0.013966, acc.: 99.62%] [G loss: 6.514575]\n",
            "15900 [D loss: 0.024821, acc.: 98.86%] [G loss: 7.260017]\n",
            "15950 [D loss: 0.016880, acc.: 100.00%] [G loss: 7.733603]\n",
            "16000 [D loss: 0.352845, acc.: 91.67%] [G loss: 19.797913]\n",
            "16050 [D loss: 0.076244, acc.: 98.48%] [G loss: 5.367565]\n",
            "16100 [D loss: 0.035851, acc.: 99.62%] [G loss: 4.912157]\n",
            "16150 [D loss: 0.021895, acc.: 100.00%] [G loss: 6.386518]\n",
            "16200 [D loss: 0.013766, acc.: 100.00%] [G loss: 6.727469]\n",
            "16250 [D loss: 0.014445, acc.: 99.62%] [G loss: 6.258309]\n",
            "16300 [D loss: 0.005034, acc.: 100.00%] [G loss: 8.160629]\n",
            "16350 [D loss: 0.227433, acc.: 92.80%] [G loss: 6.007425]\n",
            "16400 [D loss: 0.044822, acc.: 98.11%] [G loss: 5.884753]\n",
            "16450 [D loss: 0.014877, acc.: 99.62%] [G loss: 7.344096]\n",
            "16500 [D loss: 0.005355, acc.: 100.00%] [G loss: 6.642514]\n",
            "16550 [D loss: 0.008227, acc.: 100.00%] [G loss: 8.294495]\n",
            "16600 [D loss: 0.082956, acc.: 95.83%] [G loss: 9.245209]\n",
            "16650 [D loss: 0.029367, acc.: 98.86%] [G loss: 8.288427]\n",
            "16700 [D loss: 0.005641, acc.: 100.00%] [G loss: 9.477554]\n",
            "16750 [D loss: 0.008628, acc.: 99.62%] [G loss: 8.498394]\n",
            "16800 [D loss: 0.021353, acc.: 98.86%] [G loss: 10.502407]\n",
            "16850 [D loss: 0.047413, acc.: 98.48%] [G loss: 9.790365]\n",
            "16900 [D loss: 0.036400, acc.: 98.86%] [G loss: 8.128834]\n",
            "16950 [D loss: 0.003333, acc.: 100.00%] [G loss: 7.034374]\n",
            "17000 [D loss: 0.055280, acc.: 97.73%] [G loss: 7.842856]\n",
            "17050 [D loss: 0.092172, acc.: 97.35%] [G loss: 18.924599]\n",
            "17100 [D loss: 0.033932, acc.: 98.86%] [G loss: 7.491124]\n",
            "17150 [D loss: 0.003883, acc.: 100.00%] [G loss: 8.140946]\n",
            "17200 [D loss: 0.056731, acc.: 97.73%] [G loss: 7.876869]\n",
            "17250 [D loss: 0.017780, acc.: 99.24%] [G loss: 7.458097]\n",
            "17300 [D loss: 0.009660, acc.: 100.00%] [G loss: 7.126101]\n",
            "17350 [D loss: 0.003631, acc.: 100.00%] [G loss: 9.543385]\n",
            "17400 [D loss: 0.017103, acc.: 99.62%] [G loss: 8.441991]\n",
            "17450 [D loss: 0.003930, acc.: 100.00%] [G loss: 9.665177]\n",
            "17500 [D loss: 0.017412, acc.: 99.62%] [G loss: 11.128244]\n",
            "17550 [D loss: 0.007612, acc.: 100.00%] [G loss: 11.910797]\n",
            "17600 [D loss: 0.004885, acc.: 100.00%] [G loss: 10.218817]\n",
            "17650 [D loss: 0.008684, acc.: 100.00%] [G loss: 8.546561]\n",
            "17700 [D loss: 0.506987, acc.: 89.77%] [G loss: 31.799908]\n",
            "17750 [D loss: 0.023127, acc.: 99.62%] [G loss: 7.544478]\n",
            "17800 [D loss: 0.028263, acc.: 98.86%] [G loss: 6.244867]\n",
            "17850 [D loss: 0.010486, acc.: 99.62%] [G loss: 8.818624]\n",
            "17900 [D loss: 0.031627, acc.: 98.86%] [G loss: 9.304782]\n",
            "17950 [D loss: 0.030494, acc.: 98.86%] [G loss: 6.195967]\n",
            "18000 [D loss: 0.024969, acc.: 99.62%] [G loss: 7.988886]\n",
            "18050 [D loss: 0.016178, acc.: 99.62%] [G loss: 8.407475]\n",
            "18100 [D loss: 0.124064, acc.: 97.73%] [G loss: 7.291917]\n",
            "18150 [D loss: 0.037134, acc.: 98.11%] [G loss: 8.220917]\n",
            "18200 [D loss: 0.043976, acc.: 98.48%] [G loss: 8.305094]\n",
            "18250 [D loss: 0.004324, acc.: 100.00%] [G loss: 8.106352]\n",
            "18300 [D loss: 0.021909, acc.: 98.86%] [G loss: 10.634941]\n",
            "18350 [D loss: 0.032012, acc.: 98.86%] [G loss: 8.179141]\n",
            "18400 [D loss: 0.048171, acc.: 98.11%] [G loss: 7.092125]\n",
            "18450 [D loss: 0.007646, acc.: 100.00%] [G loss: 8.526715]\n",
            "18500 [D loss: 0.112270, acc.: 96.97%] [G loss: 15.282060]\n",
            "18550 [D loss: 0.053205, acc.: 97.73%] [G loss: 6.037787]\n",
            "18600 [D loss: 0.009671, acc.: 99.62%] [G loss: 8.299321]\n",
            "18650 [D loss: 0.012097, acc.: 100.00%] [G loss: 8.217671]\n",
            "18700 [D loss: 0.015472, acc.: 99.62%] [G loss: 8.737907]\n",
            "18750 [D loss: 0.019327, acc.: 100.00%] [G loss: 8.976684]\n",
            "18800 [D loss: 0.069123, acc.: 97.35%] [G loss: 11.353275]\n",
            "18850 [D loss: 0.037815, acc.: 98.86%] [G loss: 7.483750]\n",
            "18900 [D loss: 0.053778, acc.: 98.86%] [G loss: 6.838242]\n",
            "18950 [D loss: 0.023757, acc.: 99.24%] [G loss: 6.536577]\n",
            "19000 [D loss: 0.040125, acc.: 98.86%] [G loss: 8.347216]\n",
            "19050 [D loss: 0.093199, acc.: 96.21%] [G loss: 7.832931]\n",
            "19100 [D loss: 0.028385, acc.: 99.24%] [G loss: 7.958727]\n",
            "19150 [D loss: 0.028180, acc.: 98.48%] [G loss: 7.677840]\n",
            "19200 [D loss: 0.063149, acc.: 97.73%] [G loss: 7.037464]\n",
            "19250 [D loss: 0.013475, acc.: 99.62%] [G loss: 8.618915]\n",
            "19300 [D loss: 0.022014, acc.: 98.86%] [G loss: 9.098811]\n",
            "19350 [D loss: 0.027952, acc.: 98.11%] [G loss: 8.562634]\n",
            "19400 [D loss: 0.023345, acc.: 99.24%] [G loss: 11.665574]\n",
            "19450 [D loss: 0.081148, acc.: 95.83%] [G loss: 8.040820]\n",
            "19500 [D loss: 0.061775, acc.: 98.11%] [G loss: 7.807379]\n",
            "19550 [D loss: 0.033819, acc.: 98.86%] [G loss: 6.464544]\n",
            "19600 [D loss: 0.039183, acc.: 98.86%] [G loss: 6.147369]\n",
            "19650 [D loss: 0.046103, acc.: 98.11%] [G loss: 6.942360]\n",
            "19700 [D loss: 0.023400, acc.: 99.24%] [G loss: 10.721000]\n",
            "19750 [D loss: 0.085765, acc.: 96.97%] [G loss: 10.799671]\n",
            "19800 [D loss: 0.066881, acc.: 98.86%] [G loss: 6.232674]\n",
            "19850 [D loss: 0.043032, acc.: 98.86%] [G loss: 6.059806]\n",
            "19900 [D loss: 0.110431, acc.: 96.59%] [G loss: 10.782036]\n",
            "19950 [D loss: 0.037545, acc.: 98.11%] [G loss: 7.181831]\n",
            "20000 [D loss: 0.096550, acc.: 97.35%] [G loss: 5.785003]\n",
            "20050 [D loss: 0.052276, acc.: 98.48%] [G loss: 8.556223]\n",
            "20100 [D loss: 0.032676, acc.: 98.86%] [G loss: 9.685472]\n",
            "20150 [D loss: 0.034663, acc.: 97.73%] [G loss: 9.789075]\n",
            "20200 [D loss: 0.053351, acc.: 97.73%] [G loss: 9.040824]\n",
            "20250 [D loss: 0.046941, acc.: 97.73%] [G loss: 10.757779]\n",
            "20300 [D loss: 0.162904, acc.: 93.18%] [G loss: 9.124756]\n",
            "20350 [D loss: 0.106159, acc.: 95.45%] [G loss: 6.637730]\n",
            "20400 [D loss: 0.038045, acc.: 98.86%] [G loss: 6.846839]\n",
            "20450 [D loss: 0.046857, acc.: 97.35%] [G loss: 7.784358]\n",
            "20500 [D loss: 0.055936, acc.: 97.73%] [G loss: 8.172217]\n",
            "20550 [D loss: 0.092695, acc.: 96.21%] [G loss: 6.714362]\n",
            "20600 [D loss: 0.121151, acc.: 95.08%] [G loss: 5.997583]\n",
            "20650 [D loss: 0.037135, acc.: 99.24%] [G loss: 6.589232]\n",
            "20700 [D loss: 0.022290, acc.: 99.24%] [G loss: 7.496704]\n",
            "20750 [D loss: 0.043133, acc.: 98.86%] [G loss: 9.216917]\n",
            "20800 [D loss: 0.018912, acc.: 99.24%] [G loss: 8.679586]\n",
            "20850 [D loss: 0.005834, acc.: 99.62%] [G loss: 12.776386]\n",
            "20900 [D loss: 0.065691, acc.: 97.35%] [G loss: 10.736422]\n",
            "20950 [D loss: 0.061460, acc.: 97.73%] [G loss: 7.467343]\n",
            "21000 [D loss: 0.034676, acc.: 99.24%] [G loss: 7.594064]\n",
            "21050 [D loss: 0.024043, acc.: 99.62%] [G loss: 7.233558]\n",
            "21100 [D loss: 0.147335, acc.: 94.70%] [G loss: 6.899635]\n",
            "21150 [D loss: 0.038899, acc.: 98.48%] [G loss: 7.113035]\n",
            "21200 [D loss: 0.039001, acc.: 99.24%] [G loss: 8.252802]\n",
            "21250 [D loss: 0.029497, acc.: 99.62%] [G loss: 8.018991]\n",
            "21300 [D loss: 0.123789, acc.: 94.32%] [G loss: 9.839249]\n",
            "21350 [D loss: 0.072771, acc.: 97.35%] [G loss: 11.077633]\n",
            "21400 [D loss: 0.099534, acc.: 96.59%] [G loss: 8.931678]\n",
            "21450 [D loss: 0.080411, acc.: 96.97%] [G loss: 7.657289]\n",
            "21500 [D loss: 0.028602, acc.: 98.48%] [G loss: 9.967656]\n",
            "21550 [D loss: 0.118068, acc.: 95.45%] [G loss: 10.090363]\n",
            "21600 [D loss: 0.067098, acc.: 97.73%] [G loss: 7.147301]\n",
            "21650 [D loss: 0.223947, acc.: 90.91%] [G loss: 9.157791]\n",
            "21700 [D loss: 0.107788, acc.: 96.21%] [G loss: 8.162680]\n",
            "21750 [D loss: 0.044257, acc.: 98.86%] [G loss: 7.367575]\n",
            "21800 [D loss: 0.070299, acc.: 96.59%] [G loss: 6.282722]\n",
            "21850 [D loss: 0.026497, acc.: 99.24%] [G loss: 8.216336]\n",
            "21900 [D loss: 0.153548, acc.: 93.18%] [G loss: 5.400104]\n",
            "21950 [D loss: 0.075052, acc.: 97.35%] [G loss: 7.149615]\n",
            "22000 [D loss: 0.074005, acc.: 96.59%] [G loss: 9.796034]\n",
            "22050 [D loss: 0.067289, acc.: 97.73%] [G loss: 9.713631]\n",
            "22100 [D loss: 0.143014, acc.: 93.94%] [G loss: 7.790032]\n",
            "22150 [D loss: 0.063114, acc.: 96.97%] [G loss: 10.687454]\n",
            "22200 [D loss: 0.025549, acc.: 99.24%] [G loss: 9.770605]\n",
            "22250 [D loss: 0.100992, acc.: 96.21%] [G loss: 4.662771]\n",
            "22300 [D loss: 0.137374, acc.: 95.45%] [G loss: 7.279242]\n",
            "22350 [D loss: 0.036421, acc.: 98.86%] [G loss: 5.993409]\n",
            "22400 [D loss: 0.166332, acc.: 93.18%] [G loss: 5.494612]\n",
            "22450 [D loss: 0.114895, acc.: 95.08%] [G loss: 8.533238]\n",
            "22500 [D loss: 0.015243, acc.: 99.62%] [G loss: 7.944876]\n",
            "22550 [D loss: 0.117556, acc.: 96.21%] [G loss: 7.786592]\n",
            "22600 [D loss: 0.325722, acc.: 85.23%] [G loss: 5.797941]\n",
            "22650 [D loss: 0.094211, acc.: 96.21%] [G loss: 7.372665]\n",
            "22700 [D loss: 0.070471, acc.: 97.35%] [G loss: 8.092384]\n",
            "22750 [D loss: 0.082670, acc.: 96.97%] [G loss: 8.595203]\n",
            "22800 [D loss: 0.078413, acc.: 96.59%] [G loss: 9.574106]\n",
            "22850 [D loss: 0.100392, acc.: 95.45%] [G loss: 11.052889]\n",
            "22900 [D loss: 0.036918, acc.: 99.24%] [G loss: 7.592625]\n",
            "22950 [D loss: 1.019504, acc.: 71.97%] [G loss: 18.068935]\n",
            "23000 [D loss: 0.204114, acc.: 91.67%] [G loss: 7.469054]\n",
            "23050 [D loss: 0.184605, acc.: 93.56%] [G loss: 6.866720]\n",
            "23100 [D loss: 0.247768, acc.: 90.53%] [G loss: 9.695370]\n",
            "23150 [D loss: 0.134295, acc.: 93.94%] [G loss: 8.978482]\n",
            "23200 [D loss: 0.140074, acc.: 95.45%] [G loss: 7.482627]\n",
            "23250 [D loss: 0.082612, acc.: 96.97%] [G loss: 8.498115]\n",
            "23300 [D loss: 0.111861, acc.: 96.21%] [G loss: 7.650659]\n",
            "23350 [D loss: 0.158064, acc.: 93.94%] [G loss: 6.027285]\n",
            "23400 [D loss: 0.488813, acc.: 85.98%] [G loss: 10.166994]\n",
            "23450 [D loss: 0.165867, acc.: 94.32%] [G loss: 12.520873]\n",
            "23500 [D loss: 0.196280, acc.: 91.67%] [G loss: 11.956786]\n",
            "23550 [D loss: 0.080623, acc.: 97.35%] [G loss: 8.715345]\n",
            "23600 [D loss: 0.122197, acc.: 94.70%] [G loss: 7.536810]\n",
            "23650 [D loss: 0.294173, acc.: 91.29%] [G loss: 11.177116]\n",
            "23700 [D loss: 0.258954, acc.: 89.77%] [G loss: 7.252408]\n",
            "23750 [D loss: 0.128704, acc.: 95.45%] [G loss: 5.973050]\n",
            "23800 [D loss: 0.150191, acc.: 95.08%] [G loss: 7.787716]\n",
            "23850 [D loss: 0.097397, acc.: 96.21%] [G loss: 6.371360]\n",
            "23900 [D loss: 0.137692, acc.: 93.94%] [G loss: 7.072856]\n",
            "23950 [D loss: 0.142834, acc.: 93.94%] [G loss: 7.272021]\n",
            "24000 [D loss: 0.104204, acc.: 95.08%] [G loss: 6.287125]\n",
            "24050 [D loss: 0.139917, acc.: 95.08%] [G loss: 8.571501]\n",
            "24100 [D loss: 0.244651, acc.: 90.91%] [G loss: 4.880867]\n",
            "24150 [D loss: 0.100350, acc.: 95.83%] [G loss: 7.720581]\n",
            "24200 [D loss: 0.110905, acc.: 95.83%] [G loss: 7.124109]\n",
            "24250 [D loss: 0.107127, acc.: 96.21%] [G loss: 7.142517]\n",
            "24300 [D loss: 0.123234, acc.: 95.08%] [G loss: 12.068381]\n",
            "24350 [D loss: 0.078577, acc.: 97.73%] [G loss: 6.175146]\n",
            "24400 [D loss: 0.101728, acc.: 96.21%] [G loss: 8.752886]\n",
            "24450 [D loss: 0.073020, acc.: 96.97%] [G loss: 9.274975]\n",
            "24500 [D loss: 0.031073, acc.: 99.24%] [G loss: 9.871311]\n",
            "24550 [D loss: 0.059572, acc.: 98.11%] [G loss: 4.916350]\n",
            "24600 [D loss: 0.008344, acc.: 100.00%] [G loss: 6.283552]\n",
            "24650 [D loss: 0.133951, acc.: 96.21%] [G loss: 7.685712]\n",
            "24700 [D loss: 0.189759, acc.: 94.70%] [G loss: 8.287505]\n",
            "24750 [D loss: 0.137188, acc.: 94.70%] [G loss: 7.084337]\n",
            "24800 [D loss: 0.096069, acc.: 96.97%] [G loss: 6.859191]\n",
            "24850 [D loss: 0.092350, acc.: 95.45%] [G loss: 5.673933]\n",
            "24900 [D loss: 0.188108, acc.: 93.18%] [G loss: 8.405201]\n",
            "24950 [D loss: 0.119215, acc.: 95.08%] [G loss: 8.375296]\n",
            "25000 [D loss: 0.114325, acc.: 94.70%] [G loss: 7.663806]\n",
            "25050 [D loss: 0.067108, acc.: 98.11%] [G loss: 7.420820]\n",
            "25100 [D loss: 0.096641, acc.: 96.59%] [G loss: 8.041811]\n",
            "25150 [D loss: 0.035467, acc.: 98.86%] [G loss: 7.492473]\n",
            "25200 [D loss: 0.093536, acc.: 96.97%] [G loss: 8.687799]\n",
            "25250 [D loss: 0.148664, acc.: 94.70%] [G loss: 8.377029]\n",
            "25300 [D loss: 0.028963, acc.: 98.86%] [G loss: 7.767142]\n",
            "25350 [D loss: 0.097021, acc.: 96.21%] [G loss: 8.310383]\n",
            "25400 [D loss: 0.078633, acc.: 95.83%] [G loss: 8.320969]\n",
            "25450 [D loss: 0.099035, acc.: 97.35%] [G loss: 7.502344]\n",
            "25500 [D loss: 0.110658, acc.: 96.21%] [G loss: 7.059494]\n",
            "25550 [D loss: 0.049818, acc.: 98.86%] [G loss: 6.307076]\n",
            "25600 [D loss: 0.171846, acc.: 94.70%] [G loss: 6.865600]\n",
            "25650 [D loss: 0.133648, acc.: 95.45%] [G loss: 6.674704]\n",
            "25700 [D loss: 0.109290, acc.: 95.83%] [G loss: 6.819899]\n",
            "25750 [D loss: 0.143141, acc.: 94.70%] [G loss: 8.202646]\n",
            "25800 [D loss: 0.136818, acc.: 95.08%] [G loss: 4.981756]\n",
            "25850 [D loss: 0.078651, acc.: 96.97%] [G loss: 7.699533]\n",
            "25900 [D loss: 0.125706, acc.: 95.83%] [G loss: 5.318680]\n",
            "25950 [D loss: 0.054488, acc.: 98.11%] [G loss: 7.315830]\n",
            "26000 [D loss: 0.105260, acc.: 95.45%] [G loss: 6.474750]\n",
            "26050 [D loss: 0.104327, acc.: 95.08%] [G loss: 4.949650]\n",
            "26100 [D loss: 0.207339, acc.: 90.53%] [G loss: 10.316545]\n",
            "26150 [D loss: 0.046917, acc.: 98.86%] [G loss: 6.765286]\n",
            "26200 [D loss: 0.200253, acc.: 90.91%] [G loss: 6.793327]\n",
            "26250 [D loss: 0.161520, acc.: 92.42%] [G loss: 6.623160]\n",
            "26300 [D loss: 0.159975, acc.: 93.18%] [G loss: 7.128836]\n",
            "26350 [D loss: 0.155165, acc.: 95.08%] [G loss: 6.344799]\n",
            "26400 [D loss: 0.114211, acc.: 96.59%] [G loss: 8.508959]\n",
            "26450 [D loss: 0.113669, acc.: 95.83%] [G loss: 6.320431]\n",
            "26500 [D loss: 0.052727, acc.: 98.11%] [G loss: 6.732995]\n",
            "26550 [D loss: 0.142978, acc.: 94.32%] [G loss: 4.692148]\n",
            "26600 [D loss: 0.203726, acc.: 91.67%] [G loss: 8.178785]\n",
            "26650 [D loss: 0.099727, acc.: 96.59%] [G loss: 6.493673]\n",
            "26700 [D loss: 0.091126, acc.: 96.97%] [G loss: 7.980589]\n",
            "26750 [D loss: 0.087423, acc.: 96.97%] [G loss: 4.839961]\n",
            "26800 [D loss: 0.201362, acc.: 91.67%] [G loss: 7.637043]\n",
            "26850 [D loss: 0.242310, acc.: 91.67%] [G loss: 6.420494]\n",
            "26900 [D loss: 0.221745, acc.: 93.18%] [G loss: 4.896134]\n",
            "26950 [D loss: 0.094717, acc.: 97.73%] [G loss: 5.927159]\n",
            "27000 [D loss: 0.124922, acc.: 95.83%] [G loss: 5.365701]\n",
            "27050 [D loss: 0.155254, acc.: 93.56%] [G loss: 6.486201]\n",
            "27100 [D loss: 0.239784, acc.: 91.29%] [G loss: 9.039755]\n",
            "27150 [D loss: 0.199127, acc.: 93.56%] [G loss: 6.679687]\n",
            "27200 [D loss: 0.168886, acc.: 93.94%] [G loss: 5.791061]\n",
            "27250 [D loss: 0.088512, acc.: 96.59%] [G loss: 5.018772]\n",
            "27300 [D loss: 0.152934, acc.: 93.56%] [G loss: 6.589727]\n",
            "27350 [D loss: 0.120456, acc.: 95.83%] [G loss: 6.756526]\n",
            "27400 [D loss: 0.089493, acc.: 97.73%] [G loss: 5.567355]\n",
            "27450 [D loss: 0.148469, acc.: 94.70%] [G loss: 4.203616]\n",
            "27500 [D loss: 0.184470, acc.: 92.80%] [G loss: 6.611041]\n",
            "27550 [D loss: 0.255126, acc.: 91.29%] [G loss: 6.477732]\n",
            "27600 [D loss: 0.144492, acc.: 95.45%] [G loss: 6.466568]\n",
            "27650 [D loss: 0.087068, acc.: 96.59%] [G loss: 5.137834]\n",
            "27700 [D loss: 0.065237, acc.: 97.35%] [G loss: 6.414447]\n",
            "27750 [D loss: 0.208584, acc.: 92.05%] [G loss: 5.562638]\n",
            "27800 [D loss: 0.165410, acc.: 92.80%] [G loss: 5.482752]\n",
            "27850 [D loss: 0.121360, acc.: 94.70%] [G loss: 5.461125]\n",
            "27900 [D loss: 0.171611, acc.: 95.83%] [G loss: 5.633884]\n",
            "27950 [D loss: 0.132858, acc.: 95.45%] [G loss: 4.631573]\n",
            "28000 [D loss: 0.145296, acc.: 93.56%] [G loss: 5.230288]\n",
            "28050 [D loss: 0.376433, acc.: 84.09%] [G loss: 6.652733]\n",
            "28100 [D loss: 0.177870, acc.: 93.94%] [G loss: 6.532948]\n",
            "28150 [D loss: 0.136902, acc.: 94.70%] [G loss: 5.570308]\n",
            "28200 [D loss: 0.120833, acc.: 96.21%] [G loss: 4.983860]\n",
            "28250 [D loss: 0.165468, acc.: 93.18%] [G loss: 4.070094]\n",
            "28300 [D loss: 0.077240, acc.: 96.97%] [G loss: 6.972320]\n",
            "28350 [D loss: 0.214985, acc.: 92.80%] [G loss: 10.767245]\n",
            "28400 [D loss: 0.184789, acc.: 93.56%] [G loss: 6.397151]\n",
            "28450 [D loss: 0.180485, acc.: 93.56%] [G loss: 6.251301]\n",
            "28500 [D loss: 0.149012, acc.: 93.94%] [G loss: 5.625245]\n",
            "28550 [D loss: 0.195337, acc.: 93.94%] [G loss: 5.997370]\n",
            "28600 [D loss: 0.339645, acc.: 87.50%] [G loss: 4.718932]\n",
            "28650 [D loss: 0.111765, acc.: 95.45%] [G loss: 4.373568]\n",
            "28700 [D loss: 0.096551, acc.: 96.97%] [G loss: 5.460956]\n",
            "28750 [D loss: 0.109208, acc.: 95.45%] [G loss: 6.282032]\n",
            "28800 [D loss: 0.213063, acc.: 90.91%] [G loss: 7.585572]\n",
            "28850 [D loss: 0.081002, acc.: 97.73%] [G loss: 7.148299]\n",
            "28900 [D loss: 0.162957, acc.: 92.80%] [G loss: 7.075079]\n",
            "28950 [D loss: 0.079234, acc.: 96.21%] [G loss: 5.160257]\n",
            "29000 [D loss: 0.142072, acc.: 93.56%] [G loss: 5.642795]\n",
            "29050 [D loss: 0.082644, acc.: 96.97%] [G loss: 8.596966]\n",
            "29100 [D loss: 0.157081, acc.: 93.18%] [G loss: 6.825686]\n",
            "29150 [D loss: 0.284574, acc.: 87.12%] [G loss: 6.392589]\n",
            "29200 [D loss: 0.117978, acc.: 93.94%] [G loss: 5.324880]\n",
            "29250 [D loss: 0.206373, acc.: 93.18%] [G loss: 6.523585]\n",
            "29300 [D loss: 0.246618, acc.: 89.39%] [G loss: 5.921147]\n",
            "29350 [D loss: 0.088696, acc.: 97.35%] [G loss: 6.287635]\n",
            "29400 [D loss: 0.064470, acc.: 98.48%] [G loss: 4.416804]\n",
            "29450 [D loss: 0.124470, acc.: 94.70%] [G loss: 5.109660]\n",
            "29500 [D loss: 0.161551, acc.: 92.80%] [G loss: 5.670082]\n",
            "29550 [D loss: 0.098559, acc.: 95.83%] [G loss: 5.545043]\n",
            "29600 [D loss: 0.197495, acc.: 94.32%] [G loss: 7.429641]\n",
            "29650 [D loss: 0.350281, acc.: 84.85%] [G loss: 5.339921]\n",
            "29700 [D loss: 0.114197, acc.: 96.21%] [G loss: 7.718912]\n",
            "29750 [D loss: 0.188733, acc.: 93.18%] [G loss: 5.940472]\n",
            "29800 [D loss: 0.076845, acc.: 97.35%] [G loss: 5.918502]\n",
            "29850 [D loss: 0.045837, acc.: 99.24%] [G loss: 4.852284]\n",
            "29900 [D loss: 0.134790, acc.: 94.32%] [G loss: 5.832484]\n",
            "29950 [D loss: 0.117601, acc.: 96.21%] [G loss: 4.035663]\n",
            "30000 [D loss: 0.232182, acc.: 89.39%] [G loss: 5.138283]\n",
            "30050 [D loss: 0.151481, acc.: 95.45%] [G loss: 5.077113]\n",
            "30100 [D loss: 0.154984, acc.: 95.83%] [G loss: 6.932488]\n",
            "30150 [D loss: 0.156103, acc.: 93.56%] [G loss: 6.604889]\n",
            "30200 [D loss: 0.164870, acc.: 95.08%] [G loss: 5.904572]\n",
            "30250 [D loss: 0.092612, acc.: 96.21%] [G loss: 6.001174]\n",
            "30300 [D loss: 0.074946, acc.: 98.48%] [G loss: 5.183283]\n",
            "30350 [D loss: 0.242349, acc.: 90.15%] [G loss: 4.643901]\n",
            "30400 [D loss: 0.134040, acc.: 95.08%] [G loss: 7.566925]\n",
            "30450 [D loss: 0.153527, acc.: 93.18%] [G loss: 5.580270]\n",
            "30500 [D loss: 0.124774, acc.: 94.70%] [G loss: 4.106880]\n",
            "30550 [D loss: 0.112771, acc.: 96.21%] [G loss: 5.294995]\n",
            "30600 [D loss: 0.154836, acc.: 91.67%] [G loss: 3.829892]\n",
            "30650 [D loss: 0.167288, acc.: 93.94%] [G loss: 6.560258]\n",
            "30700 [D loss: 0.291302, acc.: 86.74%] [G loss: 5.252359]\n",
            "30750 [D loss: 0.287648, acc.: 89.77%] [G loss: 4.391693]\n",
            "30800 [D loss: 0.146030, acc.: 93.94%] [G loss: 6.680634]\n",
            "30850 [D loss: 0.284645, acc.: 88.26%] [G loss: 4.998048]\n",
            "30900 [D loss: 0.205205, acc.: 93.94%] [G loss: 5.904080]\n",
            "30950 [D loss: 0.151783, acc.: 95.45%] [G loss: 5.439756]\n",
            "31000 [D loss: 0.104179, acc.: 97.35%] [G loss: 5.072093]\n",
            "31050 [D loss: 0.437614, acc.: 85.98%] [G loss: 10.805018]\n",
            "31100 [D loss: 0.114019, acc.: 94.70%] [G loss: 6.600456]\n",
            "31150 [D loss: 0.126842, acc.: 95.45%] [G loss: 5.928894]\n",
            "31200 [D loss: 0.117389, acc.: 95.83%] [G loss: 7.181751]\n",
            "31250 [D loss: 0.097321, acc.: 96.59%] [G loss: 6.404362]\n",
            "31300 [D loss: 0.088976, acc.: 95.45%] [G loss: 6.147696]\n",
            "31350 [D loss: 0.179634, acc.: 93.18%] [G loss: 5.718108]\n",
            "31400 [D loss: 0.113070, acc.: 96.97%] [G loss: 6.998154]\n",
            "31450 [D loss: 0.187701, acc.: 92.80%] [G loss: 6.518610]\n",
            "31500 [D loss: 0.176605, acc.: 93.94%] [G loss: 6.269778]\n",
            "31550 [D loss: 0.101334, acc.: 95.45%] [G loss: 5.871454]\n",
            "31600 [D loss: 0.199232, acc.: 92.42%] [G loss: 4.189782]\n",
            "31650 [D loss: 0.153123, acc.: 93.56%] [G loss: 8.154054]\n",
            "31700 [D loss: 0.173874, acc.: 92.80%] [G loss: 5.592123]\n",
            "31750 [D loss: 0.188893, acc.: 92.42%] [G loss: 5.542651]\n",
            "31800 [D loss: 0.139056, acc.: 95.08%] [G loss: 6.714059]\n",
            "31850 [D loss: 0.173402, acc.: 92.80%] [G loss: 6.073536]\n",
            "31900 [D loss: 0.192860, acc.: 92.42%] [G loss: 6.012241]\n",
            "31950 [D loss: 0.194385, acc.: 91.67%] [G loss: 6.137814]\n",
            "32000 [D loss: 0.128319, acc.: 93.18%] [G loss: 5.111191]\n",
            "32050 [D loss: 0.090588, acc.: 97.73%] [G loss: 4.509367]\n",
            "32100 [D loss: 0.254106, acc.: 92.42%] [G loss: 5.423574]\n",
            "32150 [D loss: 0.211480, acc.: 91.29%] [G loss: 4.418569]\n",
            "32200 [D loss: 0.097654, acc.: 96.21%] [G loss: 6.700703]\n",
            "32250 [D loss: 0.275310, acc.: 87.50%] [G loss: 5.743629]\n",
            "32300 [D loss: 0.108700, acc.: 95.45%] [G loss: 3.771079]\n",
            "32350 [D loss: 0.116263, acc.: 95.83%] [G loss: 4.995234]\n",
            "32400 [D loss: 0.111625, acc.: 96.21%] [G loss: 4.865563]\n",
            "32450 [D loss: 0.217438, acc.: 91.29%] [G loss: 7.175934]\n",
            "32500 [D loss: 0.228186, acc.: 91.29%] [G loss: 6.947948]\n",
            "32550 [D loss: 0.133231, acc.: 94.70%] [G loss: 6.570726]\n",
            "32600 [D loss: 0.134233, acc.: 94.32%] [G loss: 5.881912]\n",
            "32650 [D loss: 0.116332, acc.: 95.45%] [G loss: 6.666985]\n",
            "32700 [D loss: 0.258625, acc.: 88.64%] [G loss: 4.124259]\n",
            "32750 [D loss: 0.220644, acc.: 91.29%] [G loss: 5.217856]\n",
            "32800 [D loss: 0.222726, acc.: 91.67%] [G loss: 5.904376]\n",
            "32850 [D loss: 0.169531, acc.: 95.83%] [G loss: 5.164711]\n",
            "32900 [D loss: 0.130950, acc.: 94.70%] [G loss: 5.385161]\n",
            "32950 [D loss: 0.217252, acc.: 90.91%] [G loss: 6.494704]\n",
            "33000 [D loss: 0.171082, acc.: 92.42%] [G loss: 4.788254]\n",
            "33050 [D loss: 0.190027, acc.: 92.42%] [G loss: 6.411222]\n",
            "33100 [D loss: 0.142212, acc.: 95.45%] [G loss: 5.110678]\n",
            "33150 [D loss: 0.219295, acc.: 91.67%] [G loss: 6.735250]\n",
            "33200 [D loss: 0.143627, acc.: 93.94%] [G loss: 6.103643]\n",
            "33250 [D loss: 0.162516, acc.: 93.94%] [G loss: 5.066119]\n",
            "33300 [D loss: 0.130344, acc.: 93.56%] [G loss: 6.258681]\n",
            "33350 [D loss: 0.175540, acc.: 93.56%] [G loss: 5.089659]\n",
            "33400 [D loss: 0.184641, acc.: 92.80%] [G loss: 4.417986]\n",
            "33450 [D loss: 0.194003, acc.: 93.18%] [G loss: 6.269197]\n",
            "33500 [D loss: 0.163121, acc.: 93.94%] [G loss: 6.187350]\n",
            "33550 [D loss: 0.159403, acc.: 92.05%] [G loss: 5.369154]\n",
            "33600 [D loss: 0.185851, acc.: 91.29%] [G loss: 4.450611]\n",
            "33650 [D loss: 0.251809, acc.: 92.05%] [G loss: 6.051204]\n",
            "33700 [D loss: 0.137684, acc.: 95.45%] [G loss: 5.037371]\n",
            "33750 [D loss: 0.205792, acc.: 91.67%] [G loss: 4.287399]\n",
            "33800 [D loss: 0.191405, acc.: 92.80%] [G loss: 4.828466]\n",
            "33850 [D loss: 0.162977, acc.: 91.67%] [G loss: 4.351457]\n",
            "33900 [D loss: 0.136655, acc.: 95.83%] [G loss: 4.627237]\n",
            "33950 [D loss: 0.217657, acc.: 92.80%] [G loss: 5.243460]\n",
            "34000 [D loss: 0.141742, acc.: 96.21%] [G loss: 6.105133]\n",
            "34050 [D loss: 0.143159, acc.: 94.70%] [G loss: 5.007294]\n",
            "34100 [D loss: 0.178745, acc.: 92.80%] [G loss: 5.473064]\n",
            "34150 [D loss: 0.209112, acc.: 92.80%] [G loss: 6.449821]\n",
            "34200 [D loss: 0.200644, acc.: 90.53%] [G loss: 5.687728]\n",
            "34250 [D loss: 0.149529, acc.: 95.08%] [G loss: 6.054726]\n",
            "34300 [D loss: 0.139381, acc.: 93.56%] [G loss: 4.730482]\n",
            "34350 [D loss: 0.169324, acc.: 91.29%] [G loss: 5.403755]\n",
            "34400 [D loss: 0.140563, acc.: 94.70%] [G loss: 4.787970]\n",
            "34450 [D loss: 0.170039, acc.: 92.05%] [G loss: 3.837116]\n",
            "34500 [D loss: 0.181209, acc.: 92.80%] [G loss: 5.365991]\n",
            "34550 [D loss: 0.210096, acc.: 92.42%] [G loss: 5.738928]\n",
            "34600 [D loss: 0.181668, acc.: 93.56%] [G loss: 5.291790]\n",
            "34650 [D loss: 0.194072, acc.: 93.56%] [G loss: 5.394190]\n",
            "34700 [D loss: 0.139293, acc.: 93.94%] [G loss: 5.502820]\n",
            "34750 [D loss: 0.134902, acc.: 94.70%] [G loss: 5.393171]\n",
            "34800 [D loss: 0.197535, acc.: 92.05%] [G loss: 4.686115]\n",
            "34850 [D loss: 0.272780, acc.: 90.91%] [G loss: 5.087396]\n",
            "34900 [D loss: 0.215500, acc.: 89.39%] [G loss: 5.609695]\n",
            "34950 [D loss: 0.174136, acc.: 93.94%] [G loss: 4.697304]\n",
            "35000 [D loss: 0.254761, acc.: 92.05%] [G loss: 4.673365]\n",
            "35050 [D loss: 0.150599, acc.: 94.70%] [G loss: 4.383128]\n",
            "35100 [D loss: 0.128363, acc.: 95.08%] [G loss: 4.883354]\n",
            "35150 [D loss: 0.184984, acc.: 92.80%] [G loss: 4.371763]\n",
            "35200 [D loss: 0.195385, acc.: 93.18%] [G loss: 5.347034]\n",
            "35250 [D loss: 0.153033, acc.: 94.70%] [G loss: 5.134967]\n",
            "35300 [D loss: 0.197579, acc.: 92.05%] [G loss: 5.934645]\n",
            "35350 [D loss: 0.151418, acc.: 93.18%] [G loss: 4.113724]\n",
            "35400 [D loss: 0.240867, acc.: 89.39%] [G loss: 3.865208]\n",
            "35450 [D loss: 0.197749, acc.: 92.05%] [G loss: 4.976894]\n",
            "35500 [D loss: 0.142276, acc.: 94.32%] [G loss: 5.692821]\n",
            "35550 [D loss: 0.117252, acc.: 96.59%] [G loss: 5.572589]\n",
            "35600 [D loss: 0.128698, acc.: 95.45%] [G loss: 6.068190]\n",
            "35650 [D loss: 0.217396, acc.: 91.29%] [G loss: 7.031843]\n",
            "35700 [D loss: 0.152111, acc.: 93.18%] [G loss: 5.417313]\n",
            "35750 [D loss: 0.233570, acc.: 90.91%] [G loss: 3.704730]\n",
            "35800 [D loss: 0.175679, acc.: 93.56%] [G loss: 5.123897]\n",
            "35850 [D loss: 0.149086, acc.: 94.32%] [G loss: 4.741797]\n",
            "35900 [D loss: 0.163420, acc.: 95.45%] [G loss: 6.405043]\n",
            "35950 [D loss: 0.146601, acc.: 94.32%] [G loss: 5.414739]\n",
            "36000 [D loss: 0.165830, acc.: 92.05%] [G loss: 4.917702]\n",
            "36050 [D loss: 0.200282, acc.: 92.80%] [G loss: 5.237012]\n",
            "36100 [D loss: 0.231449, acc.: 91.67%] [G loss: 5.474899]\n",
            "36150 [D loss: 0.189481, acc.: 92.80%] [G loss: 6.971071]\n",
            "36200 [D loss: 0.109313, acc.: 96.59%] [G loss: 5.353906]\n",
            "36250 [D loss: 0.072107, acc.: 97.35%] [G loss: 5.895761]\n",
            "36300 [D loss: 0.174696, acc.: 91.29%] [G loss: 5.050554]\n",
            "36350 [D loss: 0.160656, acc.: 93.56%] [G loss: 5.442298]\n",
            "36400 [D loss: 0.133282, acc.: 93.56%] [G loss: 4.499137]\n",
            "36450 [D loss: 0.268106, acc.: 88.26%] [G loss: 6.015836]\n",
            "36500 [D loss: 0.130692, acc.: 95.83%] [G loss: 4.852558]\n",
            "36550 [D loss: 0.199452, acc.: 91.67%] [G loss: 6.683368]\n",
            "36600 [D loss: 0.161091, acc.: 94.70%] [G loss: 5.547410]\n",
            "36650 [D loss: 0.143445, acc.: 93.56%] [G loss: 5.789096]\n",
            "36700 [D loss: 0.167556, acc.: 93.18%] [G loss: 4.586200]\n",
            "36750 [D loss: 0.108293, acc.: 96.21%] [G loss: 3.760382]\n",
            "36800 [D loss: 0.110736, acc.: 95.83%] [G loss: 5.196238]\n",
            "36850 [D loss: 0.113823, acc.: 95.83%] [G loss: 5.109906]\n",
            "36900 [D loss: 0.254787, acc.: 87.50%] [G loss: 4.075700]\n",
            "36950 [D loss: 0.184427, acc.: 92.80%] [G loss: 5.212730]\n",
            "37000 [D loss: 0.190263, acc.: 92.80%] [G loss: 4.580060]\n",
            "37050 [D loss: 0.306083, acc.: 85.98%] [G loss: 6.458201]\n",
            "37100 [D loss: 0.136892, acc.: 93.56%] [G loss: 4.280787]\n",
            "37150 [D loss: 0.175983, acc.: 92.80%] [G loss: 4.291263]\n",
            "37200 [D loss: 0.136390, acc.: 93.94%] [G loss: 4.934163]\n",
            "37250 [D loss: 0.287145, acc.: 87.88%] [G loss: 5.904026]\n",
            "37300 [D loss: 0.216081, acc.: 91.29%] [G loss: 3.927306]\n",
            "37350 [D loss: 0.285498, acc.: 87.88%] [G loss: 2.925815]\n",
            "37400 [D loss: 0.173717, acc.: 93.94%] [G loss: 4.473239]\n",
            "37450 [D loss: 0.219877, acc.: 91.67%] [G loss: 3.993504]\n",
            "37500 [D loss: 0.229869, acc.: 90.53%] [G loss: 5.854333]\n",
            "37550 [D loss: 0.282245, acc.: 85.61%] [G loss: 4.774455]\n",
            "37600 [D loss: 0.166220, acc.: 92.05%] [G loss: 4.284122]\n",
            "37650 [D loss: 0.278905, acc.: 89.02%] [G loss: 4.680683]\n",
            "37700 [D loss: 0.240012, acc.: 90.91%] [G loss: 3.039771]\n",
            "37750 [D loss: 0.230540, acc.: 90.53%] [G loss: 3.782447]\n",
            "37800 [D loss: 0.242383, acc.: 91.67%] [G loss: 5.442181]\n",
            "37850 [D loss: 0.174682, acc.: 91.67%] [G loss: 4.322627]\n",
            "37900 [D loss: 0.187293, acc.: 93.94%] [G loss: 3.650135]\n",
            "37950 [D loss: 0.172089, acc.: 92.80%] [G loss: 5.244532]\n",
            "38000 [D loss: 0.250857, acc.: 91.67%] [G loss: 4.835830]\n",
            "38050 [D loss: 0.247056, acc.: 89.02%] [G loss: 4.790935]\n",
            "38100 [D loss: 0.103449, acc.: 95.83%] [G loss: 4.008644]\n",
            "38150 [D loss: 0.172291, acc.: 91.67%] [G loss: 5.166485]\n",
            "38200 [D loss: 0.186648, acc.: 91.67%] [G loss: 4.169879]\n",
            "38250 [D loss: 0.249614, acc.: 90.53%] [G loss: 4.755126]\n",
            "38300 [D loss: 0.169519, acc.: 93.18%] [G loss: 4.788040]\n",
            "38350 [D loss: 0.135517, acc.: 95.45%] [G loss: 4.577950]\n",
            "38400 [D loss: 0.186117, acc.: 93.56%] [G loss: 3.980238]\n",
            "38450 [D loss: 0.222536, acc.: 91.67%] [G loss: 5.700387]\n",
            "38500 [D loss: 0.220661, acc.: 90.15%] [G loss: 4.119493]\n",
            "38550 [D loss: 0.241154, acc.: 90.15%] [G loss: 4.228055]\n",
            "38600 [D loss: 0.154759, acc.: 94.70%] [G loss: 4.184725]\n",
            "38650 [D loss: 0.327832, acc.: 89.02%] [G loss: 3.376898]\n",
            "38700 [D loss: 0.262696, acc.: 88.26%] [G loss: 3.336343]\n",
            "38750 [D loss: 0.177852, acc.: 92.42%] [G loss: 5.005854]\n",
            "38800 [D loss: 0.264128, acc.: 89.77%] [G loss: 4.388546]\n",
            "38850 [D loss: 0.138947, acc.: 96.97%] [G loss: 3.914634]\n",
            "38900 [D loss: 0.147935, acc.: 94.70%] [G loss: 3.852615]\n",
            "38950 [D loss: 0.155366, acc.: 94.32%] [G loss: 3.976273]\n",
            "39000 [D loss: 0.122293, acc.: 95.08%] [G loss: 3.908532]\n",
            "39050 [D loss: 0.225132, acc.: 90.15%] [G loss: 4.586566]\n",
            "39100 [D loss: 0.258853, acc.: 91.29%] [G loss: 5.438923]\n",
            "39150 [D loss: 0.288041, acc.: 89.77%] [G loss: 5.306532]\n",
            "39200 [D loss: 0.321900, acc.: 86.36%] [G loss: 4.503443]\n",
            "39250 [D loss: 0.216013, acc.: 91.29%] [G loss: 4.772471]\n",
            "39300 [D loss: 0.226025, acc.: 91.29%] [G loss: 5.506911]\n",
            "39350 [D loss: 0.279913, acc.: 88.64%] [G loss: 4.551808]\n",
            "39400 [D loss: 0.245345, acc.: 90.91%] [G loss: 4.640625]\n",
            "39450 [D loss: 0.163444, acc.: 93.18%] [G loss: 4.213834]\n",
            "39500 [D loss: 0.192499, acc.: 91.29%] [G loss: 4.291229]\n",
            "39550 [D loss: 0.227858, acc.: 90.15%] [G loss: 4.198050]\n",
            "39600 [D loss: 0.334635, acc.: 84.85%] [G loss: 5.080640]\n",
            "39650 [D loss: 0.168503, acc.: 91.29%] [G loss: 4.382529]\n",
            "39700 [D loss: 0.260605, acc.: 90.53%] [G loss: 3.850529]\n",
            "39750 [D loss: 0.284989, acc.: 87.50%] [G loss: 4.411308]\n",
            "39800 [D loss: 0.250994, acc.: 89.39%] [G loss: 5.320634]\n",
            "39850 [D loss: 0.298778, acc.: 88.64%] [G loss: 3.880327]\n",
            "39900 [D loss: 0.206748, acc.: 90.91%] [G loss: 4.181324]\n",
            "39950 [D loss: 0.224122, acc.: 90.15%] [G loss: 3.960134]\n",
            "40000 [D loss: 0.130790, acc.: 95.45%] [G loss: 4.119147]\n",
            "40050 [D loss: 0.254027, acc.: 89.02%] [G loss: 3.761949]\n",
            "40100 [D loss: 0.307553, acc.: 89.02%] [G loss: 4.938375]\n",
            "40150 [D loss: 0.225483, acc.: 92.05%] [G loss: 5.015550]\n",
            "40200 [D loss: 0.158001, acc.: 93.94%] [G loss: 4.647515]\n",
            "40250 [D loss: 0.233660, acc.: 91.67%] [G loss: 4.763697]\n",
            "40300 [D loss: 0.169699, acc.: 95.08%] [G loss: 3.965108]\n",
            "40350 [D loss: 0.292124, acc.: 87.50%] [G loss: 3.562445]\n",
            "40400 [D loss: 0.179872, acc.: 92.80%] [G loss: 4.258926]\n",
            "40450 [D loss: 0.268471, acc.: 87.50%] [G loss: 3.391151]\n",
            "40500 [D loss: 0.236593, acc.: 88.64%] [G loss: 4.047342]\n",
            "40550 [D loss: 0.188291, acc.: 91.67%] [G loss: 4.379136]\n",
            "40600 [D loss: 0.218177, acc.: 89.39%] [G loss: 4.501630]\n",
            "40650 [D loss: 0.229065, acc.: 90.53%] [G loss: 4.295786]\n",
            "40700 [D loss: 0.269313, acc.: 88.26%] [G loss: 3.949024]\n",
            "40750 [D loss: 0.202323, acc.: 91.29%] [G loss: 3.626751]\n",
            "40800 [D loss: 0.187104, acc.: 93.94%] [G loss: 3.488005]\n",
            "40850 [D loss: 0.282944, acc.: 90.53%] [G loss: 3.542097]\n",
            "40900 [D loss: 0.204139, acc.: 90.53%] [G loss: 3.769771]\n",
            "40950 [D loss: 0.262792, acc.: 88.64%] [G loss: 3.586468]\n",
            "41000 [D loss: 0.240067, acc.: 89.77%] [G loss: 3.249770]\n",
            "41050 [D loss: 0.275494, acc.: 88.26%] [G loss: 3.713984]\n",
            "41100 [D loss: 0.228083, acc.: 89.02%] [G loss: 3.440365]\n",
            "41150 [D loss: 0.197414, acc.: 91.67%] [G loss: 3.886589]\n",
            "41200 [D loss: 0.223965, acc.: 91.29%] [G loss: 3.861731]\n",
            "41250 [D loss: 0.168852, acc.: 93.18%] [G loss: 4.036917]\n",
            "41300 [D loss: 0.305084, acc.: 87.88%] [G loss: 4.491333]\n",
            "41350 [D loss: 0.202996, acc.: 90.91%] [G loss: 4.300769]\n",
            "41400 [D loss: 0.184611, acc.: 93.18%] [G loss: 4.087556]\n",
            "41450 [D loss: 0.308237, acc.: 87.50%] [G loss: 4.198678]\n",
            "41500 [D loss: 0.243844, acc.: 88.26%] [G loss: 3.886892]\n",
            "41550 [D loss: 0.192612, acc.: 91.29%] [G loss: 4.835083]\n",
            "41600 [D loss: 0.194520, acc.: 92.42%] [G loss: 3.654870]\n",
            "41650 [D loss: 0.233736, acc.: 92.05%] [G loss: 3.718456]\n",
            "41700 [D loss: 0.277746, acc.: 89.77%] [G loss: 4.033191]\n",
            "41750 [D loss: 0.384260, acc.: 82.58%] [G loss: 4.884724]\n",
            "41800 [D loss: 0.241156, acc.: 90.15%] [G loss: 5.517472]\n",
            "41850 [D loss: 0.341424, acc.: 85.98%] [G loss: 5.235189]\n",
            "41900 [D loss: 0.221465, acc.: 89.39%] [G loss: 3.363723]\n",
            "41950 [D loss: 0.224547, acc.: 91.67%] [G loss: 3.592762]\n",
            "42000 [D loss: 0.201068, acc.: 90.15%] [G loss: 3.331907]\n",
            "42050 [D loss: 0.214702, acc.: 92.05%] [G loss: 3.948477]\n",
            "42100 [D loss: 0.221165, acc.: 91.67%] [G loss: 4.356395]\n",
            "42150 [D loss: 0.316622, acc.: 86.36%] [G loss: 3.668361]\n",
            "42200 [D loss: 0.208480, acc.: 91.67%] [G loss: 4.256360]\n",
            "42250 [D loss: 0.217975, acc.: 90.15%] [G loss: 4.265626]\n",
            "42300 [D loss: 0.150111, acc.: 94.32%] [G loss: 3.736858]\n",
            "42350 [D loss: 0.252336, acc.: 90.91%] [G loss: 3.652827]\n",
            "42400 [D loss: 0.269893, acc.: 88.26%] [G loss: 3.890226]\n",
            "42450 [D loss: 0.253008, acc.: 87.88%] [G loss: 4.235071]\n",
            "42500 [D loss: 0.307624, acc.: 86.36%] [G loss: 4.006275]\n",
            "42550 [D loss: 0.215875, acc.: 90.53%] [G loss: 4.344583]\n",
            "42600 [D loss: 0.234142, acc.: 90.91%] [G loss: 4.336197]\n",
            "42650 [D loss: 0.330029, acc.: 85.98%] [G loss: 5.090562]\n",
            "42700 [D loss: 0.215135, acc.: 92.80%] [G loss: 4.003120]\n",
            "42750 [D loss: 0.305073, acc.: 89.02%] [G loss: 4.018891]\n",
            "42800 [D loss: 0.224342, acc.: 90.15%] [G loss: 3.637057]\n",
            "42850 [D loss: 0.297784, acc.: 86.36%] [G loss: 3.079120]\n",
            "42900 [D loss: 0.263834, acc.: 88.26%] [G loss: 3.166496]\n",
            "42950 [D loss: 0.418524, acc.: 84.09%] [G loss: 3.493810]\n",
            "43000 [D loss: 0.312783, acc.: 86.36%] [G loss: 3.348254]\n",
            "43050 [D loss: 0.267250, acc.: 87.88%] [G loss: 3.387530]\n",
            "43100 [D loss: 0.384916, acc.: 83.33%] [G loss: 3.193043]\n",
            "43150 [D loss: 0.251810, acc.: 89.02%] [G loss: 4.223903]\n",
            "43200 [D loss: 0.272883, acc.: 87.88%] [G loss: 3.458420]\n",
            "43250 [D loss: 0.262904, acc.: 90.15%] [G loss: 4.716097]\n",
            "43300 [D loss: 0.231449, acc.: 91.29%] [G loss: 3.447816]\n",
            "43350 [D loss: 0.305274, acc.: 85.98%] [G loss: 3.356961]\n",
            "43400 [D loss: 0.222700, acc.: 90.15%] [G loss: 4.004973]\n",
            "43450 [D loss: 0.268768, acc.: 86.36%] [G loss: 2.865342]\n",
            "43500 [D loss: 0.331524, acc.: 86.36%] [G loss: 3.574645]\n",
            "43550 [D loss: 0.374132, acc.: 82.58%] [G loss: 2.859833]\n",
            "43600 [D loss: 0.228341, acc.: 90.53%] [G loss: 4.004575]\n",
            "43650 [D loss: 0.247570, acc.: 92.42%] [G loss: 3.778907]\n",
            "43700 [D loss: 0.301392, acc.: 84.85%] [G loss: 4.084391]\n",
            "43750 [D loss: 0.279822, acc.: 89.02%] [G loss: 3.673468]\n",
            "43800 [D loss: 0.267680, acc.: 89.39%] [G loss: 3.728597]\n",
            "43850 [D loss: 0.375142, acc.: 84.47%] [G loss: 4.408195]\n",
            "43900 [D loss: 0.224266, acc.: 93.18%] [G loss: 4.482963]\n",
            "43950 [D loss: 0.378725, acc.: 81.82%] [G loss: 3.339653]\n",
            "44000 [D loss: 0.290893, acc.: 86.36%] [G loss: 3.450000]\n",
            "44050 [D loss: 0.292595, acc.: 87.50%] [G loss: 3.805991]\n",
            "44100 [D loss: 0.427129, acc.: 82.20%] [G loss: 3.894142]\n",
            "44150 [D loss: 0.260021, acc.: 87.50%] [G loss: 3.819298]\n",
            "44200 [D loss: 0.296387, acc.: 87.50%] [G loss: 4.242713]\n",
            "44250 [D loss: 0.276223, acc.: 86.74%] [G loss: 4.559905]\n",
            "44300 [D loss: 0.360948, acc.: 84.85%] [G loss: 3.167299]\n",
            "44350 [D loss: 0.294341, acc.: 85.23%] [G loss: 3.651742]\n",
            "44400 [D loss: 0.337144, acc.: 85.98%] [G loss: 4.063343]\n",
            "44450 [D loss: 0.293847, acc.: 88.26%] [G loss: 4.142122]\n",
            "44500 [D loss: 0.313553, acc.: 84.47%] [G loss: 3.455405]\n",
            "44550 [D loss: 0.362918, acc.: 84.09%] [G loss: 3.278393]\n",
            "44600 [D loss: 0.348932, acc.: 84.85%] [G loss: 3.929896]\n",
            "44650 [D loss: 0.280365, acc.: 86.74%] [G loss: 2.762120]\n",
            "44700 [D loss: 0.254391, acc.: 90.53%] [G loss: 3.790083]\n",
            "44750 [D loss: 0.261690, acc.: 88.26%] [G loss: 2.712748]\n",
            "44800 [D loss: 0.297895, acc.: 86.74%] [G loss: 3.105135]\n",
            "44850 [D loss: 0.321406, acc.: 85.61%] [G loss: 3.331485]\n",
            "44900 [D loss: 0.296154, acc.: 86.74%] [G loss: 3.827542]\n",
            "44950 [D loss: 0.342996, acc.: 88.26%] [G loss: 3.856173]\n",
            "45000 [D loss: 0.266924, acc.: 86.36%] [G loss: 4.212850]\n",
            "45050 [D loss: 0.247830, acc.: 88.64%] [G loss: 3.777688]\n",
            "45100 [D loss: 0.335548, acc.: 85.61%] [G loss: 2.564832]\n",
            "45150 [D loss: 0.343958, acc.: 85.23%] [G loss: 2.959145]\n",
            "45200 [D loss: 0.314948, acc.: 86.74%] [G loss: 3.311028]\n",
            "45250 [D loss: 0.323035, acc.: 88.26%] [G loss: 4.034356]\n",
            "45300 [D loss: 0.251017, acc.: 89.77%] [G loss: 3.520541]\n",
            "45350 [D loss: 0.238641, acc.: 90.91%] [G loss: 3.162748]\n",
            "45400 [D loss: 0.327240, acc.: 87.12%] [G loss: 3.535309]\n",
            "45450 [D loss: 0.388025, acc.: 84.85%] [G loss: 3.374532]\n",
            "45500 [D loss: 0.333693, acc.: 85.98%] [G loss: 3.693213]\n",
            "45550 [D loss: 0.385659, acc.: 82.20%] [G loss: 3.820057]\n",
            "45600 [D loss: 0.250939, acc.: 88.64%] [G loss: 3.797642]\n",
            "45650 [D loss: 0.286197, acc.: 89.02%] [G loss: 2.769299]\n",
            "45700 [D loss: 0.269248, acc.: 87.12%] [G loss: 3.837291]\n",
            "45750 [D loss: 0.327608, acc.: 85.98%] [G loss: 2.814311]\n",
            "45800 [D loss: 0.400865, acc.: 81.06%] [G loss: 3.164904]\n",
            "45850 [D loss: 0.419091, acc.: 81.44%] [G loss: 3.789029]\n",
            "45900 [D loss: 0.287139, acc.: 87.88%] [G loss: 4.696330]\n",
            "45950 [D loss: 0.383602, acc.: 81.82%] [G loss: 3.450175]\n",
            "46000 [D loss: 0.408260, acc.: 82.20%] [G loss: 2.608779]\n",
            "46050 [D loss: 0.281753, acc.: 86.74%] [G loss: 3.022377]\n",
            "46100 [D loss: 0.311767, acc.: 84.47%] [G loss: 3.354019]\n",
            "46150 [D loss: 0.265463, acc.: 88.26%] [G loss: 3.907886]\n",
            "46200 [D loss: 0.262922, acc.: 90.15%] [G loss: 3.374248]\n",
            "46250 [D loss: 0.309486, acc.: 85.23%] [G loss: 3.020282]\n",
            "46300 [D loss: 0.326586, acc.: 85.61%] [G loss: 3.204759]\n",
            "46350 [D loss: 0.228696, acc.: 89.39%] [G loss: 3.232229]\n",
            "46400 [D loss: 0.313633, acc.: 86.36%] [G loss: 3.112164]\n",
            "46450 [D loss: 0.407119, acc.: 80.68%] [G loss: 3.302642]\n",
            "46500 [D loss: 0.298796, acc.: 89.02%] [G loss: 3.105609]\n",
            "46550 [D loss: 0.257528, acc.: 88.26%] [G loss: 3.082366]\n",
            "46600 [D loss: 0.317217, acc.: 85.98%] [G loss: 4.286316]\n",
            "46650 [D loss: 0.292156, acc.: 88.26%] [G loss: 3.286335]\n",
            "46700 [D loss: 0.262393, acc.: 87.50%] [G loss: 3.699793]\n",
            "46750 [D loss: 0.295521, acc.: 84.85%] [G loss: 2.627136]\n",
            "46800 [D loss: 0.389798, acc.: 84.09%] [G loss: 3.297878]\n",
            "46850 [D loss: 0.246100, acc.: 89.39%] [G loss: 3.607846]\n",
            "46900 [D loss: 0.329717, acc.: 82.95%] [G loss: 3.657053]\n",
            "46950 [D loss: 0.309270, acc.: 87.50%] [G loss: 4.080008]\n",
            "47000 [D loss: 0.250467, acc.: 89.39%] [G loss: 3.466947]\n",
            "47050 [D loss: 0.269614, acc.: 89.39%] [G loss: 3.532180]\n",
            "47100 [D loss: 0.378733, acc.: 85.61%] [G loss: 3.295449]\n",
            "47150 [D loss: 0.335622, acc.: 85.23%] [G loss: 3.328687]\n",
            "47200 [D loss: 0.260621, acc.: 88.64%] [G loss: 3.840372]\n",
            "47250 [D loss: 0.216239, acc.: 91.67%] [G loss: 2.916499]\n",
            "47300 [D loss: 0.347314, acc.: 84.09%] [G loss: 3.695408]\n",
            "47350 [D loss: 0.321980, acc.: 85.61%] [G loss: 3.619624]\n",
            "47400 [D loss: 0.291246, acc.: 87.50%] [G loss: 3.441371]\n",
            "47450 [D loss: 0.281401, acc.: 89.02%] [G loss: 3.545206]\n",
            "47500 [D loss: 0.277465, acc.: 90.15%] [G loss: 3.369955]\n",
            "47550 [D loss: 0.296942, acc.: 87.50%] [G loss: 2.482305]\n",
            "47600 [D loss: 0.298708, acc.: 88.26%] [G loss: 3.214556]\n",
            "47650 [D loss: 0.331856, acc.: 85.98%] [G loss: 3.068178]\n",
            "47700 [D loss: 0.158749, acc.: 93.56%] [G loss: 3.862601]\n",
            "47750 [D loss: 0.316075, acc.: 86.36%] [G loss: 3.086186]\n",
            "47800 [D loss: 0.364581, acc.: 82.95%] [G loss: 3.339687]\n",
            "47850 [D loss: 0.272379, acc.: 89.39%] [G loss: 3.075043]\n",
            "47900 [D loss: 0.262544, acc.: 87.50%] [G loss: 3.179421]\n",
            "47950 [D loss: 0.296179, acc.: 87.88%] [G loss: 2.809033]\n",
            "48000 [D loss: 0.395907, acc.: 81.06%] [G loss: 2.642288]\n",
            "48050 [D loss: 0.323727, acc.: 86.36%] [G loss: 3.911056]\n",
            "48100 [D loss: 0.284303, acc.: 86.74%] [G loss: 3.537641]\n",
            "48150 [D loss: 0.319567, acc.: 84.47%] [G loss: 3.500074]\n",
            "48200 [D loss: 0.293181, acc.: 84.85%] [G loss: 3.084915]\n",
            "48250 [D loss: 0.322809, acc.: 85.23%] [G loss: 3.127772]\n",
            "48300 [D loss: 0.259051, acc.: 90.15%] [G loss: 2.897150]\n",
            "48350 [D loss: 0.247591, acc.: 89.77%] [G loss: 3.122651]\n",
            "48400 [D loss: 0.326076, acc.: 85.98%] [G loss: 3.025529]\n",
            "48450 [D loss: 0.333224, acc.: 86.36%] [G loss: 3.544112]\n",
            "48500 [D loss: 0.237520, acc.: 89.02%] [G loss: 4.108960]\n",
            "48550 [D loss: 0.394096, acc.: 82.20%] [G loss: 3.031701]\n",
            "48600 [D loss: 0.305654, acc.: 84.85%] [G loss: 2.610450]\n",
            "48650 [D loss: 0.326079, acc.: 83.71%] [G loss: 2.913500]\n",
            "48700 [D loss: 0.259725, acc.: 87.88%] [G loss: 2.445022]\n",
            "48750 [D loss: 0.256082, acc.: 87.12%] [G loss: 3.174025]\n",
            "48800 [D loss: 0.342851, acc.: 85.23%] [G loss: 2.566159]\n",
            "48850 [D loss: 0.307834, acc.: 87.50%] [G loss: 3.463945]\n",
            "48900 [D loss: 0.303405, acc.: 85.98%] [G loss: 2.867085]\n",
            "48950 [D loss: 0.333897, acc.: 85.98%] [G loss: 2.788535]\n",
            "49000 [D loss: 0.362908, acc.: 81.44%] [G loss: 3.752511]\n",
            "49050 [D loss: 0.310445, acc.: 87.50%] [G loss: 2.775866]\n",
            "49100 [D loss: 0.385194, acc.: 81.06%] [G loss: 1.962477]\n",
            "49150 [D loss: 0.388573, acc.: 84.09%] [G loss: 2.507002]\n",
            "49200 [D loss: 0.365141, acc.: 84.09%] [G loss: 3.599174]\n",
            "49250 [D loss: 0.217115, acc.: 90.91%] [G loss: 3.482852]\n",
            "49300 [D loss: 0.361824, acc.: 85.98%] [G loss: 2.694562]\n",
            "49350 [D loss: 0.332830, acc.: 87.50%] [G loss: 2.790052]\n",
            "49400 [D loss: 0.292531, acc.: 86.74%] [G loss: 2.982477]\n",
            "49450 [D loss: 0.306363, acc.: 87.12%] [G loss: 3.392070]\n",
            "49500 [D loss: 0.378126, acc.: 82.95%] [G loss: 3.083628]\n",
            "49550 [D loss: 0.322797, acc.: 86.36%] [G loss: 3.552110]\n",
            "49600 [D loss: 0.361294, acc.: 83.71%] [G loss: 2.851810]\n",
            "49650 [D loss: 0.303567, acc.: 85.61%] [G loss: 3.035518]\n",
            "49700 [D loss: 0.368752, acc.: 85.98%] [G loss: 4.057309]\n",
            "49750 [D loss: 0.259004, acc.: 90.15%] [G loss: 3.322324]\n",
            "49800 [D loss: 0.358190, acc.: 85.98%] [G loss: 2.794733]\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-34-bb93339f034e>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0mgan\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mGAN\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0mgan\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mepochs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m100000\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m132\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msample_interval\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m10000\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m<ipython-input-33-9ffc93bb6a8c>\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(self, epochs, batch_size, sample_interval)\u001b[0m\n\u001b[1;32m    167\u001b[0m             '''TODO: Posteriormente de haber realizado una actualización de los pesos del generador, generaremos un batch de\n\u001b[1;32m    168\u001b[0m                imágenes (falsas) a partir de ruido.''' \n\u001b[0;32m--> 169\u001b[0;31m             \u001b[0mgen_imgs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgenerator\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnoise\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    170\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    171\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/keras/engine/training.py\u001b[0m in \u001b[0;36m_method_wrapper\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m     86\u001b[0m       raise ValueError('{} is not supported in multi-worker mode.'.format(\n\u001b[1;32m     87\u001b[0m           method.__name__))\n\u001b[0;32m---> 88\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mmethod\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     89\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     90\u001b[0m   return tf_decorator.make_decorator(\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/keras/engine/training.py\u001b[0m in \u001b[0;36mpredict\u001b[0;34m(self, x, batch_size, verbose, steps, callbacks, max_queue_size, workers, use_multiprocessing)\u001b[0m\n\u001b[1;32m   1250\u001b[0m           \u001b[0mworkers\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mworkers\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1251\u001b[0m           \u001b[0muse_multiprocessing\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0muse_multiprocessing\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1252\u001b[0;31m           model=self)\n\u001b[0m\u001b[1;32m   1253\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1254\u001b[0m       \u001b[0;31m# Container that configures and calls `tf.keras.Callback`s.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/keras/engine/data_adapter.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, x, y, sample_weight, batch_size, steps_per_epoch, initial_epoch, epochs, shuffle, class_weight, max_queue_size, workers, use_multiprocessing, model)\u001b[0m\n\u001b[1;32m   1110\u001b[0m         \u001b[0muse_multiprocessing\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0muse_multiprocessing\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1111\u001b[0m         \u001b[0mdistribution_strategy\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mds_context\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_strategy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1112\u001b[0;31m         model=model)\n\u001b[0m\u001b[1;32m   1113\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1114\u001b[0m     \u001b[0mstrategy\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mds_context\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_strategy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/keras/engine/data_adapter.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, x, y, sample_weights, sample_weight_modes, batch_size, epochs, steps, shuffle, **kwargs)\u001b[0m\n\u001b[1;32m    326\u001b[0m     \u001b[0;31m# trigger the next permutation. On the other hand, too many simultaneous\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    327\u001b[0m     \u001b[0;31m# shuffles can contend on a hardware level and degrade all performance.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 328\u001b[0;31m     \u001b[0mindices_dataset\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mindices_dataset\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmap\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpermutation\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mprefetch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    329\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    330\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mslice_batch_indices\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mindices\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/data/ops/dataset_ops.py\u001b[0m in \u001b[0;36mmap\u001b[0;34m(self, map_func, num_parallel_calls, deterministic)\u001b[0m\n\u001b[1;32m   1619\u001b[0m     \"\"\"\n\u001b[1;32m   1620\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mnum_parallel_calls\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1621\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mMapDataset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmap_func\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpreserve_cardinality\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1622\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1623\u001b[0m       return ParallelMapDataset(\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/data/ops/dataset_ops.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, input_dataset, map_func, use_inter_op_parallelism, preserve_cardinality, use_legacy_function)\u001b[0m\n\u001b[1;32m   3958\u001b[0m   \u001b[0;34m\"\"\"A `Dataset` that maps a function over elements in its input.\"\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3959\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 3960\u001b[0;31m   def __init__(self,\n\u001b[0m\u001b[1;32m   3961\u001b[0m                \u001b[0minput_dataset\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3962\u001b[0m                \u001b[0mmap_func\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ]
    }
  ]
}